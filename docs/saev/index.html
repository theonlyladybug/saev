<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.3">
<title>saev API documentation</title>
<meta name="description" content="saev is a Python package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch â€¦">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>saev</code></h1>
</header>
<section id="section-intro">
<p>saev is a Python package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.</p>
<p>The main entrypoint to the package is in <code>__main__</code>; use <code>python -m saev --help</code> to see the options and documentation for the script.</p>
<h1 id="guide-to-training-saes-on-vision-models">Guide to Training SAEs on Vision Models</h1>
<ol>
<li>Record ViT activations and save them to disk.</li>
<li>Train SAEs on the activations.</li>
<li>Visualize the learned features from the trained SAEs.</li>
<li>(your job) Propose trends and patterns in the visualized features.</li>
<li>(your job, supported by code) Construct datasets to test your hypothesized trends.</li>
<li>Confirm/reject hypotheses using <code>probing</code> package.</li>
</ol>
<p><code><a title="saev" href="#saev">saev</a></code> helps with steps 1, 2 and 3.</p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;<code><a title="saev" href="#saev">saev</a></code> assumes you are running on NVIDIA GPUs. On a multi-GPU system, prefix your commands with <code>CUDA_VISIBLE_DEVICES=X</code> to run on GPU X.</p>
</div>
<h2 id="record-vit-activations-to-disk">Record ViT Activations to Disk</h2>
<p>To save activations to disk, we need to specify:</p>
<ol>
<li>Which model we would like to use</li>
<li>Which layers we would like to save.</li>
<li>Where on disk and how we would like to save activations.</li>
<li>Which images we want to save activations for.</li>
</ol>
<p>The <code><a title="saev.activations" href="activations.html">saev.activations</a></code> module does all of this for us.</p>
<p>Run <code>uv run python -m saev activations --help</code> to see all the configuration.</p>
<p>In practice, you might run:</p>
<pre><code class="language-sh">uv run python -m saev activations \
  --vit-family clip \
  --vit-ckpt ViT-B-32/openai \
  --d-vit 768 \
  --n-patches-per-img 49 \
  --layers -2 \
  --dump-to /local/scratch/$USER/cache/saev \
  --n-patches-per-shard 2_4000_000 \
  data:imagenet-dataset
</code></pre>
<p>This will save activations for the CLIP-pretrained model ViT-B/32, which has a residual stream dimension of 768, and has 49 patches per image (224 / 32 = 7; 7 x 7 = 49).
It will save the second-to-last layer (<code>--layer -2</code>).
It will write 2.4M patches per shard, and save shards to a new directory <code>/local/scratch$USER/cache/saev</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;A note on storage space: A ViT-B/16 will save 1.2M images x 197 patches/layer/image x 1 layer = ~240M activations, each of which take up 768 floats x 4 bytes/float = 3072 bytes, for a <strong>total of 723GB</strong> for the entire dataset. As you scale to larger models (ViT-L has 1024 dimensions, 14x14 patches are 224 patches/layer/image), recorded activations will grow even larger.</p>
</div>
<p>This script will also save a <code>metadata.json</code> file that will record the relevant metadata for these activations, which will be read by future steps.
The activations will be in <code>.bin</code> files, numbered starting from 000000.</p>
<p>To add your own models, see the guide to extending in <code><a title="saev.activations" href="activations.html">saev.activations</a></code>.</p>
<h2 id="train-saes-on-activations">Train SAEs on Activations</h2>
<p>To train an SAE, we need to specify:</p>
<ol>
<li>Which activations to use as input.</li>
<li>SAE architectural stuff.</li>
<li>Optimization-related stuff.</li>
</ol>
<p><code>The </code>saev.training` module handles this.</p>
<p>Run <code>uv run python -m saev train --help</code> to see all the configuration.</p>
<p>Continuing on from our example before, you might want to run something like:</p>
<pre><code class="language-sh">uv run python -m saev train \
  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \
  --data.layer -2 \
  --data.patches patches \
  --data.no-scale-mean \
  --data.no-scale-norm \
  --sae.d-vit 768 \
  --lr 5e-4
</code></pre>
<p><code>--data.*</code> flags describe which activations to use.</p>
<p><code>--data.shard-root</code> should point to a directory with <code>*.bin</code> files and the <code>metadata.json</code> file.
<code>--data.layer</code> specifies the layer, and <code>--data.patches</code> says that want to train on individual patch activations, rather than the [CLS] token activation.
<code>--data.no-scale-mean</code> and <code>--data.no-scale-norm</code> mean not to scale the activation mean or L2 norm.
Anthropic's and OpenAI's papers suggest normalizing these factors, but <code><a title="saev" href="#saev">saev</a></code> still has a bug with this, so I suggest not scaling these factors.</p>
<p><code>--sae.*</code> flags are about the SAE itself.</p>
<p><code>--sae.d-vit</code> is the only one you need to change; the dimension of our ViT was 768 for a ViT-B, rather than the default of 1024 for a ViT-L.</p>
<p>Finally, choose a slightly larger learning rate than the default with <code>--lr 5e-4</code>.</p>
<p>This will train one (1) sparse autoencoder on the data.
See the section on sweeps to learn how to train multiple SAEs in parallel using only a single GPU.</p>
<h2 id="visualize-the-learned-features">Visualize the Learned Features</h2>
<p>Now that you've trained an SAE, you probably want to look at its learned features.
One way to visualize an individual learned feature <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> is by picking out images that maximize the activation of feature <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span>.
Since we train SAEs on patch-level activations, we try to find the top <em>patches</em> for each feature <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span>.
Then, we pick out the images those patches correspond to and create a heatmap based on SAE activation values.</p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;More advanced forms of visualization are possible (and valuable!), but should not be included in <code><a title="saev" href="#saev">saev</a></code> unless they can be applied to every SAE/dataset combination. If you have specific visualizations, please add them to <code>contrib/</code> or another location.</p>
</div>
<p><code><a title="saev.visuals" href="visuals.html">saev.visuals</a></code> records these maximally activating images for us.
You can see all the options with <code>uv run python -m saev visuals --help</code>.</p>
<p>So you might run:</p>
<pre><code class="language-sh">uv run python -m saev visuals \
  --ckpt checkpoints/abcdefg/sae.pt \
  --dump-to /nfs/$USER/saev/webapp/abcdefg \
  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \
  --data.layer -2 \
  --data.patches patches \
  images:imagenet-dataset
</code></pre>
<p>This will record the top 128 patches, and then save the unique images among those top 128 patches for each feature in the trained SAE.
It will cache these best activations to disk, then start saving images to visualize later on.</p>
<p><code><a title="saev.interactive.features" href="interactive/features.html">saev.interactive.features</a></code> is a small web application based on <a href="https://marimo.io/">marimo</a> to interactively look at these images.</p>
<p>You can run it with <code>uv run marimo edit saev/interactive/features.py</code>.</p>
<h2 id="sweeps">Sweeps</h2>
<div class="admonition todo">
<p class="admonition-title">TODO</p>
<p>Explain how to run grid sweeps.</p>
</div>
<h2 id="training-metrics-and-visualizations">Training Metrics and Visualizations</h2>
<div class="admonition todo">
<p class="admonition-title">TODO</p>
<p>Explain how to use the <code><a title="saev.interactive.metrics" href="interactive/metrics.html">saev.interactive.metrics</a></code> notebook.</p>
</div>
<h1 id="related-work">Related Work</h1>
<p>Various papers and internet posts on training SAEs for vision.</p>
<h2 id="preprints">Preprints</h2>
<p><a href="https://arxiv.org/pdf/2410.03334">An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation</a>
* Haven't read this yet, but Hugo Fry is an author.</p>
<h2 id="lesswrong">LessWrong</h2>
<p><a href="https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2">Towards Multimodal Interpretability: Learning Sparse Interpretable Features in Vision Transformers</a>
* Trains a sparse autoencoder on the 22nd layer of a CLIP ViT-L/14. First public work training an SAE on a ViT. Finds interesting features, demonstrating that SAEs work with ViTs.</p>
<p><a href="https://www.lesswrong.com/posts/Quqekpvx8BGMMcaem/interpreting-and-steering-features-in-images">Interpreting and Steering Features in Images</a>
* Havne't read it yet.</p>
<p><a href="https://www.lesswrong.com/posts/iYFuZo9BMvr6GgMs5/case-study-interpreting-manipulating-and-controlling-clip">Case Study: Interpreting, Manipulating, and Controlling CLIP With Sparse Autoencoders</a>
* Followup to the above work; haven't read it yet.</p>
<p><a href="https://www.lesswrong.com/posts/wrznNDMRmbQABAEMH/a-suite-of-vision-sparse-autoencoders">A Suite of Vision Sparse Autoencoders</a>
* Train a sparse autoencoder on various layers using the TopK with k=32 on a CLIP ViT-L/14 trained on LAION-2B. The SAE is trained on 1.2B tokens including patch (not just [CLS]). Limited evaluation.</p>
<h1 id="reproduce">Reproduce</h1>
<p>To reproduce our findings from our preprint, you will need to train a couple SAEs on various datasets, then save visual examples so you can browse them in the notebooks.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li>Save activations for ImageNet and iNat2021 for DINOv2, CLIP and BioCLIP.</li>
<li>Train SAEs on these activation datasets.</li>
<li>Pick the best SAE checkpoints for each combination.</li>
<li>Save visualizations for those best checkpoints.</li>
</ol>
<h2 id="save-activations">Save Activations</h2>
<h2 id="train-saes">Train SAEs</h2>
<h2 id="choose-best-checkpoints">Choose Best Checkpoints</h2>
<h2 id="save-visualizations">Save Visualizations</h2>
<p>Get visuals for the iNat-trained SAEs (BioCLIP and CLIP):</p>
<pre><code class="language-sh">uv run python -m saev visuals \
  --ckpt checkpoints/$CKPT/sae.pt \
  --dump-to /$NFS/$USER/saev-visuals/$CKPT/ \
  --log-freq-range -2.0 -1.0 \
  --log-value-range -0.75 2.0 \
  --data.shard-root /local/scratch/$USER/cache/saev/$SHARDS \
  images:image-folder-dataset \
  --images.root /$NFS/$USER/datasets/inat21/train_mini/
</code></pre>
<p>Look at these visuals in the interactive notebook.</p>
<pre><code class="language-sh">uv run marimo edit
</code></pre>
<p>Then open <a href="https://localhost:2718">localhost:2718</a> in your browser and open the <code>saev/interactive/features.py</code> file.
Choose one of the checkpoints in the dropdown and click through the different neurons to find patterns in the underlying ViT.</p>
<h1 id="inference-instructions">Inference Instructions</h1>
<p>Briefly, you need to:</p>
<ol>
<li>Download a checkpoint.</li>
<li>Get the code.</li>
<li>Load the checkpoint.</li>
<li>Get activations.</li>
</ol>
<p>Details are below.</p>
<h2 id="download-a-checkpoint">Download a Checkpoint</h2>
<p>First, download an SAE checkpoint from the <a href="https://huggingface.co/collections/osunlp/sae-v-67ab8c4fdf179d117db28195">Huggingface collection</a>.</p>
<p>For instance, you can choose the SAE trained on OpenAI's CLIP ViT-B/16 with ImageNet-1K activations <a href="https://huggingface.co/osunlp/SAE_CLIP_24K_ViT-B-16_IN1K">here</a>.</p>
<p>You can use <code>wget</code> if you want:</p>
<pre><code class="language-sh">wget https://huggingface.co/osunlp/SAE_CLIP_24K_ViT-B-16_IN1K/resolve/main/sae.pt
</code></pre>
<h2 id="get-the-code">Get the Code</h2>
<p>The easiest way to do this is to clone the code:</p>
<pre><code>git clone https://github.com/OSU-NLP-Group/SAE-V
</code></pre>
<p>You can also install the package from git if you use uv (not sure about pip or cuda):</p>
<pre><code class="language-sh">uv add git+https://github.com/OSU-NLP-Group/SAE-V
</code></pre>
<p>Or clone it and install it as an editable with pip, lik <code>pip install -e .</code> in your virtual environment.</p>
<p>Then you can do things like <code>from <a title="saev" href="#saev">saev</a> import &hellip;</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you struggle to get <code><a title="saev" href="#saev">saev</a></code> installed, open an issue on <a href="https://github.com/OSU-NLP-Group/SAE-V">GitHub</a> and I will figure out how to make it easier.</p>
</div>
<h2 id="load-the-checkpoint">Load the Checkpoint</h2>
<pre><code class="language-py">import saev.nn

sae = saev.nn.load(&quot;PATH_TO_YOUR_SAE_CKPT.pt&quot;)
</code></pre>
<p>Now you have a pretrained SAE.</p>
<h2 id="get-activations">Get Activations</h2>
<p>This is the hardest part.
We need to:</p>
<ol>
<li>Pass an image into a ViT</li>
<li>Record the dense ViT activations at the same layer that the SAE was trained on.</li>
<li>Pass the activations into the SAE to get sparse activations.</li>
<li>Do something interesting with the sparse SAE activations.</li>
</ol>
<p>There are examples of this in the demo code: for <a href="https://huggingface.co/spaces/samuelstevens/saev-image-classification/blob/main/app.py#L318">classification</a> and <a href="https://huggingface.co/spaces/samuelstevens/saev-semantic-segmentation/blob/main/app.py#L222">semantic segmentation</a>.
If the permalinks change, you are looking for the <code>get_sae_latents()</code> functions in both files.</p>
<p>Below is example code to do it using the <code><a title="saev" href="#saev">saev</a></code> package.</p>
<pre><code class="language-py">import saev.nn
import saev.activations

img_transform = saev.activations.make_img_transform(&quot;clip&quot;, &quot;ViT-B-16/openai&quot;)

vit = saev.activations.make_vit(&quot;clip&quot;, &quot;ViT-B-16/openai&quot;)
recorded_vit = saev.activations.RecordedVisionTransformer(vit, 196, True, [10])

img = Image.open(&quot;example.jpg&quot;)

x = img_transform(img)
# Add a batch dimension
x = x[None, ...]
_, vit_acts = recorded_vit(x)
# Re-select the only element in the batch, and ignore the CLS token.
vit_acts = vit_acts[0, 1:, :]

x_hat, f_x, loss = sae(vit_acts)
</code></pre>
<p>Now you have the reconstructed x (<code>x_hat</code>) and the sparse representation of all patches in the image (<code>f_x</code>).</p>
<p>You might select the dimensions with maximal values for each patch and see what other images are maximimally activating.</p>
<div class="admonition todo">
<p class="admonition-title">TODO</p>
<p>Provide documentation for how get maximally activating images.</p>
</div>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="saev.activations" href="activations.html">saev.activations</a></code></dt>
<dd>
<div class="desc"><p>To save lots of activations, we want to do things in parallel, with lots of slurm jobs, and save multiple files, rather than just one â€¦</p></div>
</dd>
<dt><code class="name"><a title="saev.app" href="app/index.html">saev.app</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="saev.colors" href="colors.html">saev.colors</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="saev.config" href="config.html">saev.config</a></code></dt>
<dd>
<div class="desc"><p>All configs for all saev jobs â€¦</p></div>
</dd>
<dt><code class="name"><a title="saev.helpers" href="helpers.html">saev.helpers</a></code></dt>
<dd>
<div class="desc"><p>Useful helpers for <code><a title="saev" href="#saev">saev</a></code>.</p></div>
</dd>
<dt><code class="name"><a title="saev.imaging" href="imaging.html">saev.imaging</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="saev.interactive" href="interactive/index.html">saev.interactive</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="saev.nn" href="nn.html">saev.nn</a></code></dt>
<dd>
<div class="desc"><p>Neural network architectures for sparse autoencoders.</p></div>
</dd>
<dt><code class="name"><a title="saev.test_activations" href="test_activations.html">saev.test_activations</a></code></dt>
<dd>
<div class="desc"><p>Test that the cached activations are actually correct.
These tests are quite slow</p></div>
</dd>
<dt><code class="name"><a title="saev.test_config" href="test_config.html">saev.test_config</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="saev.test_nn" href="test_nn.html">saev.test_nn</a></code></dt>
<dd>
<div class="desc"><p>Uses <a href="">hypothesis</a> and <a href="https://hypothesis-torch.readthedocs.io/en/stable/compatability/">hypothesis-torch</a> to generate test cases to compare our â€¦</p></div>
</dd>
<dt><code class="name"><a title="saev.test_training" href="test_training.html">saev.test_training</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="saev.test_visuals" href="test_visuals.html">saev.test_visuals</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="saev.training" href="training.html">saev.training</a></code></dt>
<dd>
<div class="desc"><p>Trains many SAEs in parallel to amortize the cost of loading a single batch of data over many SAE training runs.</p></div>
</dd>
<dt><code class="name"><a title="saev.visuals" href="visuals.html">saev.visuals</a></code></dt>
<dd>
<div class="desc"><p>There is some important notation used only in this file to dramatically shorten variable names â€¦</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul>
<li><a href="#guide-to-training-saes-on-vision-models">Guide to Training SAEs on Vision Models</a><ul>
<li><a href="#record-vit-activations-to-disk">Record ViT Activations to Disk</a></li>
<li><a href="#train-saes-on-activations">Train SAEs on Activations</a></li>
<li><a href="#visualize-the-learned-features">Visualize the Learned Features</a></li>
<li><a href="#sweeps">Sweeps</a></li>
<li><a href="#training-metrics-and-visualizations">Training Metrics and Visualizations</a></li>
</ul>
</li>
<li><a href="#related-work">Related Work</a><ul>
<li><a href="#preprints">Preprints</a></li>
<li><a href="#lesswrong">LessWrong</a></li>
</ul>
</li>
<li><a href="#reproduce">Reproduce</a><ul>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#save-activations">Save Activations</a></li>
<li><a href="#train-saes">Train SAEs</a></li>
<li><a href="#choose-best-checkpoints">Choose Best Checkpoints</a></li>
<li><a href="#save-visualizations">Save Visualizations</a></li>
</ul>
</li>
<li><a href="#inference-instructions">Inference Instructions</a><ul>
<li><a href="#download-a-checkpoint">Download a Checkpoint</a></li>
<li><a href="#get-the-code">Get the Code</a></li>
<li><a href="#load-the-checkpoint">Load the Checkpoint</a></li>
<li><a href="#get-activations">Get Activations</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="saev.activations" href="activations.html">saev.activations</a></code></li>
<li><code><a title="saev.app" href="app/index.html">saev.app</a></code></li>
<li><code><a title="saev.colors" href="colors.html">saev.colors</a></code></li>
<li><code><a title="saev.config" href="config.html">saev.config</a></code></li>
<li><code><a title="saev.helpers" href="helpers.html">saev.helpers</a></code></li>
<li><code><a title="saev.imaging" href="imaging.html">saev.imaging</a></code></li>
<li><code><a title="saev.interactive" href="interactive/index.html">saev.interactive</a></code></li>
<li><code><a title="saev.nn" href="nn.html">saev.nn</a></code></li>
<li><code><a title="saev.test_activations" href="test_activations.html">saev.test_activations</a></code></li>
<li><code><a title="saev.test_config" href="test_config.html">saev.test_config</a></code></li>
<li><code><a title="saev.test_nn" href="test_nn.html">saev.test_nn</a></code></li>
<li><code><a title="saev.test_training" href="test_training.html">saev.test_training</a></code></li>
<li><code><a title="saev.test_visuals" href="test_visuals.html">saev.test_visuals</a></code></li>
<li><code><a title="saev.training" href="training.html">saev.training</a></code></li>
<li><code><a title="saev.visuals" href="visuals.html">saev.visuals</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.3</a>.</p>
</footer>
</body>
</html>
