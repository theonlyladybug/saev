Module saev
===========
saev is a Python package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.

The main entrypoint to the package is in `__main__`; use `python -m saev --help` to see the options and documentation for the script.

# Guide to Training SAEs on Vision Models

1. Record ViT activations and save them to disk.
2. Train SAEs on the activations.
3. Visualize the learned features from the trained SAEs.
4. (your job) Propose trends and patterns in the visualized features.
5. (your job, supported by code) Construct datasets to test your hypothesized trends.
6. Confirm/reject hypotheses using `probing` package.

`saev` helps with steps 1, 2 and 3.

.. note:: `saev` assumes you are running on NVIDIA GPUs. On a multi-GPU system, prefix your commands with `CUDA_VISIBLE_DEVICES=X` to run on GPU X.

## Record ViT Activations to Disk

To save activations to disk, we need to specify:

1. Which model we would like to use
2. Which layers we would like to save.
3. Where on disk and how we would like to save activations.
4. Which images we want to save activations for.

The `saev.activations` module does all of this for us.

Run `uv run python -m saev activations --help` to see all the configuration.

In practice, you might run:

```sh
uv run python -m saev activations \
  --vit-family clip \
  --vit-ckpt ViT-B-32/openai \
  --d-vit 768 \
  --n-patches-per-img 49 \
  --layers -2 \
  --dump-to /local/scratch/$USER/cache/saev \
  --n-patches-per-shard 2_4000_000 \
  data:imagenet-dataset
```

This will save activations for the CLIP-pretrained model ViT-B/32, which has a residual stream dimension of 768, and has 49 patches per image (224 / 32 = 7; 7 x 7 = 49).
It will save the second-to-last layer (`--layer -2`).
It will write 2.4M patches per shard, and save shards to a new directory `/local/scratch$USER/cache/saev`.

.. note:: A note on storage space: A ViT-B/16 will save 1.2M images x 197 patches/layer/image x 1 layer = ~240M activations, each of which take up 768 floats x 4 bytes/float = 3072 bytes, for a **total of 723GB** for the entire dataset. As you scale to larger models (ViT-L has 1024 dimensions, 14x14 patches are 224 patches/layer/image), recorded activations will grow even larger.

This script will also save a `metadata.json` file that will record the relevant metadata for these activations, which will be read by future steps.
The activations will be in `.bin` files, numbered starting from 000000.

To add your own models, see the guide to extending in `saev.activations`.

## Train SAEs on Activations

To train an SAE, we need to specify:

1. Which activations to use as input.
2. SAE architectural stuff.
3. Optimization-related stuff.

`The `saev.training` module handles this.

Run `uv run python -m saev train --help` to see all the configuration.

Continuing on from our example before, you might want to run something like:

```sh
uv run python -m saev train \
  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \
  --data.layer -2 \
  --data.patches patches \
  --data.no-scale-mean \
  --data.no-scale-norm \
  --sae.d-vit 768 \
  --lr 5e-4
```

`--data.*` flags describe which activations to use.

`--data.shard-root` should point to a directory with `*.bin` files and the `metadata.json` file.
`--data.layer` specifies the layer, and `--data.patches` says that want to train on individual patch activations, rather than the [CLS] token activation.
`--data.no-scale-mean` and `--data.no-scale-norm` mean not to scale the activation mean or L2 norm.
Anthropic's and OpenAI's papers suggest normalizing these factors, but `saev` still has a bug with this, so I suggest not scaling these factors.

`--sae.*` flags are about the SAE itself.

`--sae.d-vit` is the only one you need to change; the dimension of our ViT was 768 for a ViT-B, rather than the default of 1024 for a ViT-L.

Finally, choose a slightly larger learning rate than the default with `--lr 5e-4`.

This will train one (1) sparse autoencoder on the data.
See the section on sweeps to learn how to train multiple SAEs in parallel using only a single GPU.

## Visualize the Learned Features

Now that you've trained an SAE, you probably want to look at its learned features.
One way to visualize an individual learned feature \(f\) is by picking out images that maximize the activation of feature \(f\).
Since we train SAEs on patch-level activations, we try to find the top *patches* for each feature \(f\).
Then, we pick out the images those patches correspond to and create a heatmap based on SAE activation values.

.. note:: More advanced forms of visualization are possible (and valuable!), but should not be included in `saev` unless they can be applied to every SAE/dataset combination. If you have specific visualizations, please add them to `contrib/` or another location.

`saev.visuals` records these maximally activating images for us.
You can see all the options with `uv run python -m saev visuals --help`.

So you might run:

```sh
uv run python -m saev visuals \
  --ckpt checkpoints/abcdefg/sae.pt \
  --dump-to /nfs/$USER/saev/webapp/abcdefg \
  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \
  --data.layer -2 \
  --data.patches patches \
  images:imagenet-dataset
```

This will record the top 128 patches, and then save the unique images among those top 128 patches for each feature in the trained SAE.
It will cache these best activations to disk, then start saving images to visualize later on.

`saev.interactive.features` is a small web application based on [marimo](https://marimo.io/) to interactively look at these images.

You can run it with `uv run marimo edit saev/interactive/features.py`.

## Sweeps

.. todo:: Explain how to run grid sweeps.

## Training Metrics and Visualizations

.. todo:: Explain how to use the `saev.interactive.metrics` notebook.

# Related Work

Various papers and internet posts on training SAEs for vision.

## Preprints

[An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation](https://arxiv.org/pdf/2410.03334)
* Haven't read this yet, but Hugo Fry is an author.

## LessWrong

[Towards Multimodal Interpretability: Learning Sparse Interpretable Features in Vision Transformers](https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2)
* Trains a sparse autoencoder on the 22nd layer of a CLIP ViT-L/14. First public work training an SAE on a ViT. Finds interesting features, demonstrating that SAEs work with ViTs.

[Interpreting and Steering Features in Images](https://www.lesswrong.com/posts/Quqekpvx8BGMMcaem/interpreting-and-steering-features-in-images)
* Havne't read it yet.

[Case Study: Interpreting, Manipulating, and Controlling CLIP With Sparse Autoencoders](https://www.lesswrong.com/posts/iYFuZo9BMvr6GgMs5/case-study-interpreting-manipulating-and-controlling-clip)
* Followup to the above work; haven't read it yet.

[A Suite of Vision Sparse Autoencoders](https://www.lesswrong.com/posts/wrznNDMRmbQABAEMH/a-suite-of-vision-sparse-autoencoders)
* Train a sparse autoencoder on various layers using the TopK with k=32 on a CLIP ViT-L/14 trained on LAION-2B. The SAE is trained on 1.2B tokens including patch (not just [CLS]). Limited evaluation.

# Reproduce

To reproduce our findings from our preprint, you will need to train a couple SAEs on various datasets, then save visual examples so you can browse them in the notebooks.

## Table of Contents

1. Save activations for ImageNet and iNat2021 for DINOv2, CLIP and BioCLIP.
2. Train SAEs on these activation datasets.
3. Pick the best SAE checkpoints for each combination.
4. Save visualizations for those best checkpoints.

## Save Activations

## Train SAEs

## Choose Best Checkpoints

## Save Visualizations

Get visuals for the iNat-trained SAEs (BioCLIP and CLIP):

```sh
uv run python -m saev visuals \
  --ckpt checkpoints/$CKPT/sae.pt \
  --dump-to /$NFS/$USER/saev-visuals/$CKPT/ \
  --log-freq-range -2.0 -1.0 \
  --log-value-range -0.75 2.0 \
  --data.shard-root /local/scratch/$USER/cache/saev/$SHARDS \
  images:image-folder-dataset \
  --images.root /$NFS/$USER/datasets/inat21/train_mini/
```

Look at these visuals in the interactive notebook.

```sh
uv run marimo edit
```

Then open [localhost:2718](https://localhost:2718) in your browser and open the `saev/interactive/features.py` file.
Choose one of the checkpoints in the dropdown and click through the different neurons to find patterns in the underlying ViT.

# Inference Instructions

Briefly, you need to:

1. Download a checkpoint.
2. Get the code.
3. Load the checkpoint.
4. Get activations.

Details are below.

## Download a Checkpoint

First, download an SAE checkpoint from the [Huggingface collection](https://huggingface.co/collections/osunlp/sae-v-67ab8c4fdf179d117db28195).

For instance, you can choose the SAE trained on OpenAI's CLIP ViT-B/16 with ImageNet-1K activations [here](https://huggingface.co/osunlp/SAE_CLIP_24K_ViT-B-16_IN1K).

You can use `wget` if you want:

```sh
wget https://huggingface.co/osunlp/SAE_CLIP_24K_ViT-B-16_IN1K/resolve/main/sae.pt
```

## Get the Code

The easiest way to do this is to clone the code:

```
git clone https://github.com/OSU-NLP-Group/SAE-V
```

You can also install the package from git if you use uv (not sure about pip or cuda):

```sh
uv add git+https://github.com/OSU-NLP-Group/SAE-V
```

Or clone it and install it as an editable with pip, lik `pip install -e .` in your virtual environment.

Then you can do things like `from saev import ...`.

.. note::
  If you struggle to get `saev` installed, open an issue on [GitHub](https://github.com/OSU-NLP-Group/SAE-V) and I will figure out how to make it easier.

## Load the Checkpoint

```py
import saev.nn

sae = saev.nn.load("PATH_TO_YOUR_SAE_CKPT.pt")
```

Now you have a pretrained SAE.

## Get Activations

This is the hardest part.
We need to:

1. Pass an image into a ViT
2. Record the dense ViT activations at the same layer that the SAE was trained on.
3. Pass the activations into the SAE to get sparse activations.
4. Do something interesting with the sparse SAE activations.

There are examples of this in the demo code: for [classification](https://huggingface.co/spaces/samuelstevens/saev-image-classification/blob/main/app.py#L318) and [semantic segmentation](https://huggingface.co/spaces/samuelstevens/saev-semantic-segmentation/blob/main/app.py#L222).
If the permalinks change, you are looking for the `get_sae_latents()` functions in both files.

Below is example code to do it using the `saev` package.

```py
import saev.nn
import saev.activations

img_transform = saev.activations.make_img_transform("clip", "ViT-B-16/openai")

vit = saev.activations.make_vit("clip", "ViT-B-16/openai")
recorded_vit = saev.activations.RecordedVisionTransformer(vit, 196, True, [10])

img = Image.open("example.jpg")

x = img_transform(img)
# Add a batch dimension
x = x[None, ...]
_, vit_acts = recorded_vit(x)
# Re-select the only element in the batch, and ignore the CLS token.
vit_acts = vit_acts[0, 1:, :]

x_hat, f_x, loss = sae(vit_acts)
```

Now you have the reconstructed x (`x_hat`) and the sparse representation of all patches in the image (`f_x`).

You might select the dimensions with maximal values for each patch and see what other images are maximimally activating.

.. todo::
  Provide documentation for how get maximally activating images.

Sub-modules
-----------
* saev.activations
* saev.app
* saev.colors
* saev.config
* saev.helpers
* saev.imaging
* saev.interactive
* saev.nn
* saev.test_activations
* saev.test_config
* saev.test_nn
* saev.test_training
* saev.test_visuals
* saev.training
* saev.visuals

Module saev.activations
=======================
To save lots of activations, we want to do things in parallel, with lots of slurm jobs, and save multiple files, rather than just one.

This module handles that additional complexity.

Conceptually, activations are either thought of as

1. A single [n_imgs x n_layers x (n_patches + 1), d_vit] tensor. This is a *dataset*
2. Multiple [n_imgs_per_shard, n_layers, (n_patches + 1), d_vit] tensors. This is a set of sharded activations.

Functions
---------

`get_acts_dir(cfg: saev.config.Activations) ‑> str`
:   Return the activations directory based on the relevant values of a config.
    Also saves a metadata.json file to that directory for human reference.
    
    Args:
        cfg: Config for experiment.
    
    Returns:
        Directory to where activations should be dumped/loaded from.

`get_dataloader(cfg: saev.config.Activations, *, img_transform=None)`
:   Gets the dataloader for the current experiment; delegates dataloader construction to dataset-specific functions.
    
    Args:
        cfg: Experiment config.
        img_transform: Image transform to be applied to each image.
    
    Returns:
        A PyTorch Dataloader that yields dictionaries with `'image'` keys containing image batches.

`get_dataset(cfg: saev.config.ImagenetDataset | saev.config.ImageFolderDataset | saev.config.Ade20kDataset, *, img_transform)`
:   Gets the dataset for the current experiment; delegates construction to dataset-specific functions.
    
    Args:
        cfg: Experiment config.
        img_transform: Image transform to be applied to each image.
    
    Returns:
        A dataset that has dictionaries with `'image'`, `'index'`, `'target'`, and `'label'` keys containing examples.

`get_default_dataloader(cfg: saev.config.Activations, *, img_transform: <class 'collections.abc.Callable'>) ‑> torch.utils.data.dataloader.DataLoader`
:   Get a dataloader for a default map-style dataset.
    
    Args:
        cfg: Config.
        img_transform: Image transform to be applied to each image.
    
    Returns:
        A PyTorch Dataloader that yields dictionaries with `'image'` keys containing image batches, `'index'` keys containing original dataset indices and `'label'` keys containing label batches.

`main(cfg: saev.config.Activations)`
:   Args:
        cfg: Config for activations.

`make_img_transform(vit_family: str, vit_ckpt: str) ‑> <class 'collections.abc.Callable'>`
:   

`make_vit(vit_family: str, vit_ckpt: str)`
:   

`setup(cfg: saev.config.Activations)`
:   Run dataset-specific setup. These setup functions can assume they are the only job running, but they should be idempotent; they should be safe (and ideally cheap) to run multiple times in a row.

`setup_ade20k(cfg: saev.config.Activations)`
:   

`setup_imagefolder(cfg: saev.config.Activations)`
:   

`setup_imagenet(cfg: saev.config.Activations)`
:   

`worker_fn(cfg: saev.config.Activations)`
:   Args:
        cfg: Config for activations.

Classes
-------

`Ade20k(cfg: saev.config.Ade20kDataset, *, img_transform: collections.abc.Callable | None = None, seg_transform: collections.abc.Callable | None = <function Ade20k.<lambda>>)`
:   An abstract class representing a :class:`Dataset`.
    
    All datasets that represent a map from keys to data samples should subclass
    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
    data sample for a given key. Subclasses could also optionally overwrite
    :meth:`__len__`, which is expected to return the size of the dataset by many
    :class:`~torch.utils.data.Sampler` implementations and the default options
    of :class:`~torch.utils.data.DataLoader`. Subclasses could also
    optionally implement :meth:`__getitems__`, for speedup batched samples
    loading. This method accepts list of indices of samples of batch and returns
    list of samples.
    
    .. note::
      :class:`~torch.utils.data.DataLoader` by default constructs an index
      sampler that yields integral indices.  To make it work with a map-style
      dataset with non-integral indices/keys, a custom sampler must be provided.

    ### Ancestors (in MRO)

    * torch.utils.data.dataset.Dataset
    * typing.Generic

    ### Class variables

    `Sample`
    :

    `samples: list[saev.activations.Ade20k.Sample]`
    :

`Clip(vit_ckpt: str)`
:   Base class for all neural network modules.
    
    Your models should also subclass this class.
    
    Modules can also contain other Modules, allowing to nest them in
    a tree structure. You can assign the submodules as regular attributes::
    
        import torch.nn as nn
        import torch.nn.functional as F
    
        class Model(nn.Module):
            def __init__(self) -> None:
                super().__init__()
                self.conv1 = nn.Conv2d(1, 20, 5)
                self.conv2 = nn.Conv2d(20, 20, 5)
    
            def forward(self, x):
                x = F.relu(self.conv1(x))
                return F.relu(self.conv2(x))
    
    Submodules assigned in this way will be registered, and will have their
    parameters converted too when you call :meth:`to`, etc.
    
    .. note::
        As per the example above, an ``__init__()`` call to the parent class
        must be made before assignment on the child.
    
    :ivar training: Boolean represents whether this module is in training or
                    evaluation mode.
    :vartype training: bool
    
    Initialize internal Module state, shared by both nn.Module and ScriptModule.

    ### Ancestors (in MRO)

    * torch.nn.modules.module.Module

    ### Methods

    `forward(self, batch: jaxtyping.Float[Tensor, 'batch 3 width height']) ‑> jaxtyping.Float[Tensor, 'batch patches dim']`
    :   Define the computation performed at every call.
        
        Should be overridden by all subclasses.
        
        .. note::
            Although the recipe for forward pass needs to be defined within
            this function, one should call the :class:`Module` instance afterwards
            instead of this since the former takes care of running the
            registered hooks while the latter silently ignores them.

    `get_patches(self, cfg: saev.config.Activations) ‑> slice`
    :

    `get_residuals(self) ‑> list[torch.nn.modules.module.Module]`
    :

`Dataset(cfg: saev.config.DataLoad)`
:   Dataset of activations from disk.

    ### Ancestors (in MRO)

    * torch.utils.data.dataset.Dataset
    * typing.Generic

    ### Class variables

    `Example`
    :   Individual example.

    `act_mean: jaxtyping.Float[Tensor, 'd_vit']`
    :   Mean activation.

    `cfg: saev.config.DataLoad`
    :   Configuration; set via CLI args.

    `layer_index: int`
    :   Layer index into the shards if we are choosing a specific layer.

    `metadata: saev.activations.Metadata`
    :   Activations metadata; automatically loaded from disk.

    `scalar: float`
    :   Normalizing scalar such that ||x / scalar ||_2 ~= sqrt(d_vit).

    ### Instance variables

    `d_vit: int`
    :   Dimension of the underlying vision transformer's embedding space.

    ### Methods

    `get_img_patches(self, i: int) ‑> jaxtyping.Float[ndarray, 'n_layers all_patches d_vit']`
    :

    `get_shard_patches(self)`
    :

    `transform(self, act: jaxtyping.Float[ndarray, 'd_vit']) ‑> jaxtyping.Float[Tensor, 'd_vit']`
    :   Apply a scalar normalization so the mean squared L2 norm is same as d_vit. This is from 'Scaling Monosemanticity':
        
        > As a preprocessing step we apply a scalar normalization to the model activations so their average squared L2 norm is the residual stream dimension
        
        So we divide by self.scalar which is the datasets (approximate) L2 mean before normalization divided by sqrt(d_vit).

`DinoV2(vit_ckpt: str)`
:   Base class for all neural network modules.
    
    Your models should also subclass this class.
    
    Modules can also contain other Modules, allowing to nest them in
    a tree structure. You can assign the submodules as regular attributes::
    
        import torch.nn as nn
        import torch.nn.functional as F
    
        class Model(nn.Module):
            def __init__(self) -> None:
                super().__init__()
                self.conv1 = nn.Conv2d(1, 20, 5)
                self.conv2 = nn.Conv2d(20, 20, 5)
    
            def forward(self, x):
                x = F.relu(self.conv1(x))
                return F.relu(self.conv2(x))
    
    Submodules assigned in this way will be registered, and will have their
    parameters converted too when you call :meth:`to`, etc.
    
    .. note::
        As per the example above, an ``__init__()`` call to the parent class
        must be made before assignment on the child.
    
    :ivar training: Boolean represents whether this module is in training or
                    evaluation mode.
    :vartype training: bool
    
    Initialize internal Module state, shared by both nn.Module and ScriptModule.

    ### Ancestors (in MRO)

    * torch.nn.modules.module.Module

    ### Methods

    `forward(self, batch: jaxtyping.Float[Tensor, 'batch 3 width height']) ‑> jaxtyping.Float[Tensor, 'batch patches dim']`
    :   Define the computation performed at every call.
        
        Should be overridden by all subclasses.
        
        .. note::
            Although the recipe for forward pass needs to be defined within
            this function, one should call the :class:`Module` instance afterwards
            instead of this since the former takes care of running the
            registered hooks while the latter silently ignores them.

    `get_patches(self, n_patches_per_img: int) ‑> slice`
    :

    `get_residuals(self) ‑> list[torch.nn.modules.module.Module]`
    :

`ImageFolder(root: str | pathlib.Path, transform: Callable | None = None, target_transform: Callable | None = None, loader: Callable[[str], Any] = <function default_loader>, is_valid_file: Callable[[str], bool] | None = None, allow_empty: bool = False)`
:   A generic data loader where the images are arranged in this way by default: ::
    
        root/dog/xxx.png
        root/dog/xxy.png
        root/dog/[...]/xxz.png
    
        root/cat/123.png
        root/cat/nsdf3.png
        root/cat/[...]/asd932_.png
    
    This class inherits from :class:`~torchvision.datasets.DatasetFolder` so
    the same methods can be overridden to customize the dataset.
    
    Args:
        root (str or ``pathlib.Path``): Root directory path.
        transform (callable, optional): A function/transform that takes in a PIL image
            and returns a transformed version. E.g, ``transforms.RandomCrop``
        target_transform (callable, optional): A function/transform that takes in the
            target and transforms it.
        loader (callable, optional): A function to load an image given its path.
        is_valid_file (callable, optional): A function that takes path of an Image file
            and check if the file is a valid file (used to check of corrupt files)
        allow_empty(bool, optional): If True, empty folders are considered to be valid classes.
            An error is raised on empty folders if False (default).
    
     Attributes:
        classes (list): List of the class names sorted alphabetically.
        class_to_idx (dict): Dict with items (class_name, class_index).
        imgs (list): List of (image path, class_index) tuples

    ### Ancestors (in MRO)

    * torchvision.datasets.folder.ImageFolder
    * torchvision.datasets.folder.DatasetFolder
    * torchvision.datasets.vision.VisionDataset
    * torch.utils.data.dataset.Dataset
    * typing.Generic

`Imagenet(cfg: saev.config.ImagenetDataset, *, img_transform=None)`
:   An abstract class representing a :class:`Dataset`.
    
    All datasets that represent a map from keys to data samples should subclass
    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
    data sample for a given key. Subclasses could also optionally overwrite
    :meth:`__len__`, which is expected to return the size of the dataset by many
    :class:`~torch.utils.data.Sampler` implementations and the default options
    of :class:`~torch.utils.data.DataLoader`. Subclasses could also
    optionally implement :meth:`__getitems__`, for speedup batched samples
    loading. This method accepts list of indices of samples of batch and returns
    list of samples.
    
    .. note::
      :class:`~torch.utils.data.DataLoader` by default constructs an index
      sampler that yields integral indices.  To make it work with a map-style
      dataset with non-integral indices/keys, a custom sampler must be provided.

    ### Ancestors (in MRO)

    * torch.utils.data.dataset.Dataset
    * typing.Generic

    ### Descendants

    * saev.app.data.VipsImagenet

`Metadata(vit_family: str, vit_ckpt: str, layers: tuple[int, ...], n_patches_per_img: int, cls_token: bool, d_vit: int, seed: int, n_imgs: int, n_patches_per_shard: int, data: str)`
:   Metadata(vit_family: str, vit_ckpt: str, layers: tuple[int, ...], n_patches_per_img: int, cls_token: bool, d_vit: int, seed: int, n_imgs: int, n_patches_per_shard: int, data: str)

    ### Class variables

    `cls_token: bool`
    :

    `d_vit: int`
    :

    `data: str`
    :

    `layers: tuple[int, ...]`
    :

    `n_imgs: int`
    :

    `n_patches_per_img: int`
    :

    `n_patches_per_shard: int`
    :

    `seed: int`
    :

    `vit_ckpt: str`
    :

    `vit_family: str`
    :

    ### Static methods

    `from_cfg(cls, cfg: saev.config.Activations) ‑> saev.activations.Metadata`
    :

    `load(cls, fpath) ‑> saev.activations.Metadata`
    :

    ### Instance variables

    `hash: str`
    :

    ### Methods

    `dump(self, fpath)`
    :

`Moondream2(vit_ckpt: str)`
:   Moondream2 has 14x14 pixel patches. For a 378x378 image (as we use here), this is 27x27 patches for a total of 729, with no [CLS] token.
    
    Initialize internal Module state, shared by both nn.Module and ScriptModule.

    ### Ancestors (in MRO)

    * torch.nn.modules.module.Module

    ### Methods

    `forward(self, batch: jaxtyping.Float[Tensor, 'batch 3 width height']) ‑> jaxtyping.Float[Tensor, 'batch patches dim']`
    :   Define the computation performed at every call.
        
        Should be overridden by all subclasses.
        
        .. note::
            Although the recipe for forward pass needs to be defined within
            this function, one should call the :class:`Module` instance afterwards
            instead of this since the former takes care of running the
            registered hooks while the latter silently ignores them.

    `get_patches(self, cfg: saev.config.Activations) ‑> slice`
    :

    `get_residuals(self) ‑> list[torch.nn.modules.module.Module]`
    :

`RecordedVisionTransformer(vit: torch.nn.modules.module.Module, n_patches_per_img: int, cls_token: bool, layers: list[int])`
:   Base class for all neural network modules.
    
    Your models should also subclass this class.
    
    Modules can also contain other Modules, allowing to nest them in
    a tree structure. You can assign the submodules as regular attributes::
    
        import torch.nn as nn
        import torch.nn.functional as F
    
        class Model(nn.Module):
            def __init__(self) -> None:
                super().__init__()
                self.conv1 = nn.Conv2d(1, 20, 5)
                self.conv2 = nn.Conv2d(20, 20, 5)
    
            def forward(self, x):
                x = F.relu(self.conv1(x))
                return F.relu(self.conv2(x))
    
    Submodules assigned in this way will be registered, and will have their
    parameters converted too when you call :meth:`to`, etc.
    
    .. note::
        As per the example above, an ``__init__()`` call to the parent class
        must be made before assignment on the child.
    
    :ivar training: Boolean represents whether this module is in training or
                    evaluation mode.
    :vartype training: bool
    
    Initialize internal Module state, shared by both nn.Module and ScriptModule.

    ### Ancestors (in MRO)

    * torch.nn.modules.module.Module

    ### Instance variables

    `activations: jaxtyping.Float[Tensor, 'batch n_layers all_patches dim']`
    :

    ### Methods

    `forward(self, batch: jaxtyping.Float[Tensor, 'batch 3 width height']) ‑> tuple[jaxtyping.Float[Tensor, 'batch patches dim'], jaxtyping.Float[Tensor, '...']]`
    :   Define the computation performed at every call.
        
        Should be overridden by all subclasses.
        
        .. note::
            Although the recipe for forward pass needs to be defined within
            this function, one should call the :class:`Module` instance afterwards
            instead of this since the former takes care of running the
            registered hooks while the latter silently ignores them.

    `hook(self, module, args: tuple, output: jaxtyping.Float[Tensor, 'batch n_layers dim']) ‑> None`
    :

    `reset(self)`
    :

`ShardWriter(cfg: saev.config.Activations)`
:   ShardWriter is a stateful object that handles sharded activation writing to disk.

    ### Class variables

    `acts: jaxtyping.Float[ndarray, 'n_imgs_per_shard n_layers all_patches d_vit'] | None`
    :

    `acts_path: str`
    :

    `filled: int`
    :

    `root: str`
    :

    `shape: tuple[int, int, int, int]`
    :

    `shard: int`
    :

    ### Methods

    `flush(self) ‑> None`
    :

    `next_shard(self) ‑> None`
    :

`Siglip(vit_ckpt: str)`
:   Base class for all neural network modules.
    
    Your models should also subclass this class.
    
    Modules can also contain other Modules, allowing to nest them in
    a tree structure. You can assign the submodules as regular attributes::
    
        import torch.nn as nn
        import torch.nn.functional as F
    
        class Model(nn.Module):
            def __init__(self) -> None:
                super().__init__()
                self.conv1 = nn.Conv2d(1, 20, 5)
                self.conv2 = nn.Conv2d(20, 20, 5)
    
            def forward(self, x):
                x = F.relu(self.conv1(x))
                return F.relu(self.conv2(x))
    
    Submodules assigned in this way will be registered, and will have their
    parameters converted too when you call :meth:`to`, etc.
    
    .. note::
        As per the example above, an ``__init__()`` call to the parent class
        must be made before assignment on the child.
    
    :ivar training: Boolean represents whether this module is in training or
                    evaluation mode.
    :vartype training: bool
    
    Initialize internal Module state, shared by both nn.Module and ScriptModule.

    ### Ancestors (in MRO)

    * torch.nn.modules.module.Module

    ### Methods

    `forward(self, batch: jaxtyping.Float[Tensor, 'batch 3 width height']) ‑> jaxtyping.Float[Tensor, 'batch patches dim']`
    :   Define the computation performed at every call.
        
        Should be overridden by all subclasses.
        
        .. note::
            Although the recipe for forward pass needs to be defined within
            this function, one should call the :class:`Module` instance afterwards
            instead of this since the former takes care of running the
            registered hooks while the latter silently ignores them.

    `get_patches(self, cfg: saev.config.Activations) ‑> slice`
    :

    `get_residuals(self) ‑> list[torch.nn.modules.module.Module]`
    :

Namespace saev.app
==================

Sub-modules
-----------
* saev.app.data
* saev.app.modeling

Module saev.app.data
====================

Functions
---------

`get_datasets()`
:   

`get_img_v_raw(key: str, i: int) ‑> tuple[pyvips.vimage.Image, str]`
:   Get raw image and processed label from dataset.
    
    Returns:
        Tuple of pyvips.Image and classname.

`pil_to_vips(img_p: PIL.Image.Image) ‑> pyvips.vimage.Image`
:   Convert a PIL Image to a pyvips Image.

`to_sized(img_v_raw: pyvips.vimage.Image, min_px: int, crop_px: tuple[int, int]) ‑> pyvips.vimage.Image`
:   Convert raw vips image to standard model input size (resize + crop).

`vips_to_base64(img_v: pyvips.vimage.Image) ‑> str`
:   

Classes
-------

`VipsImageFolder(root: str, transform: Callable | None = None, target_transform: Callable | None = None)`
:   Clone of ImageFolder that returns pyvips.Image instead of PIL.Image.Image.

    ### Ancestors (in MRO)

    * torchvision.datasets.folder.ImageFolder
    * torchvision.datasets.folder.DatasetFolder
    * torchvision.datasets.vision.VisionDataset
    * torch.utils.data.dataset.Dataset
    * typing.Generic

`VipsImagenet(cfg: saev.config.ImagenetDataset, *, img_transform=None)`
:   An abstract class representing a :class:`Dataset`.
    
    All datasets that represent a map from keys to data samples should subclass
    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
    data sample for a given key. Subclasses could also optionally overwrite
    :meth:`__len__`, which is expected to return the size of the dataset by many
    :class:`~torch.utils.data.Sampler` implementations and the default options
    of :class:`~torch.utils.data.DataLoader`. Subclasses could also
    optionally implement :meth:`__getitems__`, for speedup batched samples
    loading. This method accepts list of indices of samples of batch and returns
    list of samples.
    
    .. note::
      :class:`~torch.utils.data.DataLoader` by default constructs an index
      sampler that yields integral indices.  To make it work with a map-style
      dataset with non-integral indices/keys, a custom sampler must be provided.

    ### Ancestors (in MRO)

    * saev.activations.Imagenet
    * torch.utils.data.dataset.Dataset
    * typing.Generic

Module saev.app.modeling
========================

Functions
---------

`get_model_lookup() ‑> dict[str, saev.app.modeling.Config]`
:   

Classes
-------

`Config(key: str, vit_family: str, vit_ckpt: str, sae_ckpt: str, tensor_dpath: pathlib.Path, dataset_name: str, acts_cfg: saev.config.DataLoad)`
:   Configuration for a Vision Transformer (ViT) and Sparse Autoencoder (SAE) model pair.
    
    Stores paths and configuration needed to load and run a specific ViT+SAE combination.

    ### Class variables

    `acts_cfg: saev.config.DataLoad`
    :   Which activations to load for normalizing.

    `dataset_name: str`
    :   Which dataset to use.

    `key: str`
    :   The lookup key.

    `sae_ckpt: str`
    :   Identifier for the SAE checkpoint to load.

    `tensor_dpath: pathlib.Path`
    :   Directory containing precomputed tensors for this model combination.

    `vit_ckpt: str`
    :   Checkpoint identifier for the ViT model, either as HuggingFace path or model/checkpoint pair.

    `vit_family: str`
    :   The family of ViT model, e.g. 'clip' for CLIP models.

    ### Instance variables

    `wrapped_cfg: saev.config.Activations`
    :

Module saev.colors
==================

Module saev.config
==================
All configs for all saev jobs.

## Import Times

This module should be very fast to import so that `python main.py --help` is fast.
This means that the top-level imports should not include big packages like numpy, torch, etc.
For example, `TreeOfLife.n_imgs` imports numpy when it's needed, rather than importing it at the top level.

Also contains code for expanding configs with lists into lists of configs (grid search).
Might be expanded in the future to support pseudo-random sampling from distributions to support random hyperparameter search, as in [this file](https://github.com/samuelstevens/sax/blob/main/sax/sweep.py).

Functions
---------

`expand(config: dict[str, object]) ‑> Iterator[dict[str, object]]`
:   Expands dicts with (nested) lists into a list of (nested) dicts.

`grid(cfg: saev.config.Train, sweep_dct: dict[str, object]) ‑> tuple[list[saev.config.Train], list[str]]`
:   

Classes
-------

`Activations(data: saev.config.ImagenetDataset | saev.config.ImageFolderDataset | saev.config.Ade20kDataset = <factory>, dump_to: str = './shards', vit_family: Literal['clip', 'siglip', 'dinov2', 'moondream2'] = 'clip', vit_ckpt: str = 'ViT-L-14/openai', vit_batch_size: int = 1024, n_workers: int = 8, d_vit: int = 1024, vit_layers: list[int] = <factory>, n_patches_per_img: int = 256, cls_token: bool = True, n_patches_per_shard: int = 2400000, seed: int = 42, ssl: bool = True, device: str = 'cuda', slurm: bool = False, slurm_acct: str = 'PAS2136', log_to: str = './logs')`
:   Configuration for calculating and saving ViT activations.

    ### Class variables

    `cls_token: bool`
    :   Whether the model has a [CLS] token.

    `d_vit: int`
    :   Dimension of the ViT activations (depends on model).

    `data: saev.config.ImagenetDataset | saev.config.ImageFolderDataset | saev.config.Ade20kDataset`
    :   Which dataset to use.

    `device: str`
    :   Which device to use.

    `dump_to: str`
    :   Where to write shards.

    `log_to: str`
    :   Where to log Slurm job stdout/stderr.

    `n_patches_per_img: int`
    :   Number of ViT patches per image (depends on model).

    `n_patches_per_shard: int`
    :   Number of activations per shard; 2.4M is approximately 10GB for 1024-dimensional 4-byte activations.

    `n_workers: int`
    :   Number of dataloader workers.

    `seed: int`
    :   Random seed.

    `slurm: bool`
    :   Whether to use `submitit` to run jobs on a Slurm cluster.

    `slurm_acct: str`
    :   Slurm account string.

    `ssl: bool`
    :   Whether to use SSL.

    `vit_batch_size: int`
    :   Batch size for ViT inference.

    `vit_ckpt: str`
    :   Specific model checkpoint.

    `vit_family: Literal['clip', 'siglip', 'dinov2', 'moondream2']`
    :   Which model family.

    `vit_layers: list[int]`
    :   Which layers to save. By default, the second-to-last layer.

`Ade20kDataset(root: str = './data/ade20k', split: Literal['training', 'validation'] = 'training')`
:   

    ### Class variables

    `root: str`
    :   Where the class folders with images are stored.

    `split: Literal['training', 'validation']`
    :   Data split.

    ### Instance variables

    `n_imgs: int`
    :

`DataLoad(shard_root: str = './shards', patches: Literal['cls', 'patches', 'meanpool'] = 'patches', layer: int | Literal['all', 'meanpool'] = -2, clamp: float = 100000.0, n_random_samples: int = 524288, scale_mean: bool | str = True, scale_norm: bool | str = True)`
:   Configuration for loading activation data from disk.

    ### Class variables

    `clamp: float`
    :   Maximum value for activations; activations will be clamped to within [-clamp, clamp]`.

    `layer: int | Literal['all', 'meanpool']`
    :   .. todo: document this field.

    `n_random_samples: int`
    :   Number of random samples used to calculate approximate dataset means at startup.

    `patches: Literal['cls', 'patches', 'meanpool']`
    :   Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'patches' indicates it will return all patches. 'meanpool' returns the mean of all image patches.

    `scale_mean: bool | str`
    :   Whether to subtract approximate dataset means from examples. If a string, manually load from the filepath.

    `scale_norm: bool | str`
    :   Whether to scale average dataset norm to sqrt(d_vit). If a string, manually load from the filepath.

    `shard_root: str`
    :   Directory with .bin shards and a metadata.json file.

`ImageFolderDataset(root: str = './data/split')`
:   Configuration for a generic image folder dataset.

    ### Class variables

    `root: str`
    :   Where the class folders with images are stored.

    ### Instance variables

    `n_imgs: int`
    :   Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires walking the directory structure. If you need to reference this number very often, cache it in a local variable.

`ImagenetDataset(name: str = 'ILSVRC/imagenet-1k', split: str = 'train')`
:   Configuration for HuggingFace Imagenet.

    ### Class variables

    `name: str`
    :   Dataset name on HuggingFace. Don't need to change this..

    `split: str`
    :   Dataset split. For the default ImageNet-1K dataset, can either be 'train', 'validation' or 'test'.

    ### Instance variables

    `n_imgs: int`
    :   Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires loading the dataset. If you need to reference this number very often, cache it in a local variable.

`SparseAutoencoder(d_vit: int = 1024, exp_factor: int = 16, sparsity_coeff: float = 0.0004, n_reinit_samples: int = 524288, ghost_grads: bool = False, remove_parallel_grads: bool = True, normalize_w_dec: bool = True, seed: int = 0)`
:   SparseAutoencoder(d_vit: int = 1024, exp_factor: int = 16, sparsity_coeff: float = 0.0004, n_reinit_samples: int = 524288, ghost_grads: bool = False, remove_parallel_grads: bool = True, normalize_w_dec: bool = True, seed: int = 0)

    ### Class variables

    `d_vit: int`
    :

    `exp_factor: int`
    :   Expansion factor for SAE.

    `ghost_grads: bool`
    :   Whether to use ghost grads.

    `n_reinit_samples: int`
    :   Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean.

    `normalize_w_dec: bool`
    :   Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation.

    `remove_parallel_grads: bool`
    :   Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic.

    `seed: int`
    :   Random seed.

    `sparsity_coeff: float`
    :   How much to weight sparsity loss term.

    ### Instance variables

    `d_sae: int`
    :

`Train(data: saev.config.DataLoad = <factory>, n_workers: int = 32, n_patches: int = 100000000, sae: saev.config.SparseAutoencoder = <factory>, n_sparsity_warmup: int = 0, lr: float = 0.0004, n_lr_warmup: int = 500, sae_batch_size: int = 16384, track: bool = True, wandb_project: str = 'saev', tag: str = '', log_every: int = 25, ckpt_path: str = './checkpoints', device: Literal['cuda', 'cpu'] = 'cuda', seed: int = 42, slurm: bool = False, slurm_acct: str = 'PAS2136', log_to: str = './logs')`
:   Configuration for training a sparse autoencoder on a vision transformer.

    ### Class variables

    `ckpt_path: str`
    :   Where to save checkpoints.

    `data: saev.config.DataLoad`
    :   Data configuration

    `device: Literal['cuda', 'cpu']`
    :   Hardware device.

    `log_every: int`
    :   How often to log to WandB.

    `log_to: str`
    :   Where to log Slurm job stdout/stderr.

    `lr: float`
    :   Learning rate.

    `n_lr_warmup: int`
    :   Number of learning rate warmup steps.

    `n_patches: int`
    :   Number of SAE training examples.

    `n_sparsity_warmup: int`
    :   Number of sparsity coefficient warmup steps.

    `n_workers: int`
    :   Number of dataloader workers.

    `sae: saev.config.SparseAutoencoder`
    :   SAE configuration.

    `sae_batch_size: int`
    :   Batch size for SAE training.

    `seed: int`
    :   Random seed.

    `slurm: bool`
    :   Whether to use `submitit` to run jobs on a Slurm cluster.

    `slurm_acct: str`
    :   Slurm account string.

    `tag: str`
    :   Tag to add to WandB run.

    `track: bool`
    :   Whether to track with WandB.

    `wandb_project: str`
    :   WandB project name.

`Visuals(ckpt: str = './checkpoints/sae.pt', data: saev.config.DataLoad = <factory>, images: saev.config.ImagenetDataset | saev.config.ImageFolderDataset | saev.config.Ade20kDataset = <factory>, top_k: int = 128, n_workers: int = 16, topk_batch_size: int = 16384, sae_batch_size: int = 16384, epsilon: float = 1e-09, sort_by: Literal['cls', 'img', 'patch'] = 'patch', device: str = 'cuda', dump_to: str = './data', log_freq_range: tuple[float, float] = (-6.0, -2.0), log_value_range: tuple[float, float] = (-1.0, 1.0), include_latents: list[int] = <factory>, n_distributions: int = 25, percentile: int = 99, n_latents: int = 400, seed: int = 42)`
:   Configuration for generating visuals from trained SAEs.

    ### Class variables

    `ckpt: str`
    :   Path to the sae.pt file.

    `data: saev.config.DataLoad`
    :   Data configuration.

    `device: str`
    :   Which accelerator to use.

    `dump_to: str`
    :   Where to save data.

    `epsilon: float`
    :   Value to add to avoid log(0).

    `images: saev.config.ImagenetDataset | saev.config.ImageFolderDataset | saev.config.Ade20kDataset`
    :   Which images to use.

    `include_latents: list[int]`
    :   Latents to always include, no matter what.

    `log_freq_range: tuple[float, float]`
    :   Log10 frequency range for which to save images.

    `log_value_range: tuple[float, float]`
    :   Log10 frequency range for which to save images.

    `n_distributions: int`
    :   Number of features to save distributions for.

    `n_latents: int`
    :   Maximum number of latents to save images for.

    `n_workers: int`
    :   Number of dataloader workers.

    `percentile: int`
    :   Percentile to estimate for outlier detection.

    `sae_batch_size: int`
    :   Batch size for SAE inference.

    `seed: int`
    :   Random seed.

    `sort_by: Literal['cls', 'img', 'patch']`
    :   How to find the top k images. 'cls' picks images where the SAE latents of the ViT's [CLS] token are maximized without any patch highligting. 'img' picks images that maximize the sum of an SAE latent over all patches in the image, highlighting the patches. 'patch' pickes images that maximize an SAE latent over all patches (not summed), highlighting the patches and only showing unique images.

    `top_k: int`
    :   How many images per SAE feature to store.

    `topk_batch_size: int`
    :   Number of examples to apply top-k op to.

    ### Instance variables

    `distributions_fpath: str`
    :

    `mean_values_fpath: str`
    :

    `percentiles_fpath: str`
    :

    `root: str`
    :

    `sparsity_fpath: str`
    :

    `top_img_i_fpath: str`
    :

    `top_patch_i_fpath: str`
    :

    `top_values_fpath: str`
    :

Module saev.helpers
===================
Useful helpers for `saev`.

Functions
---------

`flattened(dct: dict[str, object], *, sep: str = '.') ‑> dict[str, str | int | float | bool | None]`
:   Flatten a potentially nested dict to a single-level dict with `.`-separated keys.

`get(dct: dict[str, object], key: str, *, sep: str = '.') ‑> object`
:   

`get_cache_dir() ‑> str`
:   Get cache directory from environment variables, defaulting to the current working directory (.)
    
    Returns:
        A path to a cache directory (might not exist yet).

Classes
-------

`progress(it, *, every: int = 10, desc: str = 'progress', total: int = 0)`
:   Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.
    
    Args:
        it: Iterable to wrap.
        every: How many iterations between logging progress.
        desc: What to name the logger.
        total: If non-zero, how long the iterable is.

Module saev.imaging
===================

Functions
---------

`add_highlights(img: PIL.Image.Image, patches: jaxtyping.Float[ndarray, 'n_patches'], *, upper: float | None = None, opacity: float = 0.9) ‑> PIL.Image.Image`
:

Namespace saev.interactive
==========================

Sub-modules
-----------
* saev.interactive.features
* saev.interactive.metrics

Module saev.interactive.features
================================

Module saev.interactive.metrics
===============================

Module saev.nn
==============
Neural network architectures for sparse autoencoders.

Functions
---------

`dump(fpath: str, sae: saev.nn.SparseAutoencoder)`
:   Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).
    
    Arguments:
        fpath: filepath to save checkpoint to.
        sae: sparse autoencoder checkpoint to save.

`load(fpath: str, *, device: str = 'cpu') ‑> saev.nn.SparseAutoencoder`
:   Loads a sparse autoencoder from disk.

`ref_mse(x_hat: jaxtyping.Float[Tensor, '*d'], x: jaxtyping.Float[Tensor, '*d'], norm: bool = True) ‑> jaxtyping.Float[Tensor, '*d']`
:   

`safe_mse(x_hat: jaxtyping.Float[Tensor, '*batch d'], x: jaxtyping.Float[Tensor, '*batch d'], norm: bool = False) ‑> jaxtyping.Float[Tensor, '*batch d']`
:   

Classes
-------

`Loss(mse: jaxtyping.Float[Tensor, ''], sparsity: jaxtyping.Float[Tensor, ''], ghost_grad: jaxtyping.Float[Tensor, ''], l0: jaxtyping.Float[Tensor, ''], l1: jaxtyping.Float[Tensor, ''])`
:   The composite loss terms for an autoencoder training batch.

    ### Ancestors (in MRO)

    * builtins.tuple

    ### Instance variables

    `ghost_grad: jaxtyping.Float[Tensor, '']`
    :   Ghost gradient loss, if any.

    `l0: jaxtyping.Float[Tensor, '']`
    :   L0 magnitude of hidden activations.

    `l1: jaxtyping.Float[Tensor, '']`
    :   L1 magnitude of hidden activations.

    `loss: jaxtyping.Float[Tensor, '']`
    :   Total loss.

    `mse: jaxtyping.Float[Tensor, '']`
    :   Reconstruction loss (mean squared error).

    `sparsity: jaxtyping.Float[Tensor, '']`
    :   Sparsity loss, typically lambda * L1.

`SparseAutoencoder(cfg: saev.config.SparseAutoencoder)`
:   Sparse auto-encoder (SAE) using L1 sparsity penalty.
    
    Initialize internal Module state, shared by both nn.Module and ScriptModule.

    ### Ancestors (in MRO)

    * torch.nn.modules.module.Module

    ### Class variables

    `cfg: saev.config.SparseAutoencoder`
    :

    ### Methods

    `decode(self, f_x: jaxtyping.Float[Tensor, 'batch d_sae']) ‑> jaxtyping.Float[Tensor, 'batch d_model']`
    :

    `forward(self, x: jaxtyping.Float[Tensor, 'batch d_model']) ‑> tuple[jaxtyping.Float[Tensor, 'batch d_model'], jaxtyping.Float[Tensor, 'batch d_sae'], saev.nn.Loss]`
    :   Given x, calculates the reconstructed x_hat, the intermediate activations f_x, and the loss.
        
        Arguments:
            x: a batch of ViT activations.

    `init_b_dec(self, vit_acts: jaxtyping.Float[Tensor, 'n d_vit'])`
    :

    `normalize_w_dec(self)`
    :   Set W_dec to unit-norm columns.

    `remove_parallel_grads(self)`
    :   Update grads so that they remove the parallel component
            (d_sae, d_vit) shape

Module saev.test_activations
============================
Test that the cached activations are actually correct.
These tests are quite slow

Functions
---------

`test_dataloader_batches()`
:   

`test_shard_writer_and_dataset_e2e()`
:

Module saev.test_config
=======================

Functions
---------

`test_expand()`
:   

`test_expand_multiple()`
:   

`test_expand_nested()`
:   

`test_expand_nested_and_unnested()`
:   

`test_expand_nested_and_unnested_backwards()`
:   

`test_expand_two_fields()`
:

Module saev.test_nn
===================
Uses [hypothesis]() and [hypothesis-torch](https://hypothesis-torch.readthedocs.io/en/stable/compatability/) to generate test cases to compare our normalized MSE implementation to a reference MSE implementation.

Functions
---------

`test_safe_mse_hypothesis() ‑> None`
:   

`test_safe_mse_large_x()`
:   

`test_safe_mse_nonzero()`
:   

`test_safe_mse_same()`
:   

`test_safe_mse_zero_x_hat()`
:

Module saev.test_training
=========================

Functions
---------

`test_split_cfgs_no_bad_keys()`
:   

`test_split_cfgs_on_multiple_keys_with_multiple_per_key()`
:   

`test_split_cfgs_on_single_key()`
:   

`test_split_cfgs_on_single_key_with_multiple_per_key()`
:

Module saev.test_visuals
========================

Functions
---------

`test_gather_batched_small()`
:

Module saev.training
====================
Trains many SAEs in parallel to amortize the cost of loading a single batch of data over many SAE training runs.

Functions
---------

`evaluate(cfgs: list[saev.config.Train], saes: torch.nn.modules.container.ModuleList) ‑> list[saev.training.EvalMetrics]`
:   Evaluates SAE quality by counting the number of dead features and the number of dense features.
    Also makes histogram plots to help human qualitative comparison.
    
    .. todo:: Develop automatic methods to use histogram and feature frequencies to evaluate quality with a single number.

`init_b_dec_batched(saes: torch.nn.modules.container.ModuleList, dataset: saev.activations.Dataset)`
:   

`main(cfgs: list[saev.config.Train]) ‑> list[str]`
:   

`make_hashable(obj)`
:   

`make_saes(cfgs: list[saev.config.SparseAutoencoder]) ‑> tuple[torch.nn.modules.container.ModuleList, list[dict[str, object]]]`
:   

`split_cfgs(cfgs: list[saev.config.Train]) ‑> list[list[saev.config.Train]]`
:   Splits configs into groups that can be parallelized.
    
    Arguments:
        A list of configs from a sweep file.
    
    Returns:
        A list of lists, where the configs in each sublist do not differ in any keys that are in `CANNOT_PARALLELIZE`. This means that each sublist is a valid "parallel" set of configs for `train`.

`train(cfgs: list[saev.config.Train]) ‑> tuple[torch.nn.modules.container.ModuleList, saev.training.ParallelWandbRun, int]`
:   Explicitly declare the optimizer, schedulers, dataloader, etc outside of `main` so that all the variables are dropped from scope and can be garbage collected.

Classes
-------

`BatchLimiter(dataloader: torch.utils.data.dataloader.DataLoader, n_samples: int)`
:   Limits the number of batches to only return `n_samples` total samples.

`EvalMetrics(l0: float, l1: float, mse: float, n_dead: int, n_almost_dead: int, n_dense: int, freqs: jaxtyping.Float[Tensor, 'd_sae'], mean_values: jaxtyping.Float[Tensor, 'd_sae'], almost_dead_threshold: float, dense_threshold: float)`
:   Results of evaluating a trained SAE on a datset.

    ### Class variables

    `almost_dead_threshold: float`
    :   Threshold for an "almost dead" neuron.

    `dense_threshold: float`
    :   Threshold for a dense neuron.

    `freqs: jaxtyping.Float[Tensor, 'd_sae']`
    :   How often each feature fired.

    `l0: float`
    :   Mean L0 across all examples.

    `l1: float`
    :   Mean L1 across all examples.

    `mean_values: jaxtyping.Float[Tensor, 'd_sae']`
    :   The mean value for each feature when it did fire.

    `mse: float`
    :   Mean MSE across all examples.

    `n_almost_dead: int`
    :   Number of neurons that fired on fewer than `almost_dead_threshold` of examples.

    `n_dead: int`
    :   Number of neurons that never fired on any example.

    `n_dense: int`
    :   Number of neurons that fired on more than `dense_threshold` of examples.

    ### Methods

    `for_wandb(self) ‑> dict[str, int | float]`
    :

`ParallelWandbRun(project: str, cfgs: list[saev.config.Train], mode: str, tags: list[str])`
:   Inspired by https://community.wandb.ai/t/is-it-possible-to-log-to-multiple-runs-simultaneously/4387/3.

    ### Methods

    `finish(self) ‑> list[str]`
    :

    `log(self, metrics: list[dict[str, object]], *, step: int)`
    :

`Scheduler()`
:   

    ### Descendants

    * saev.training.Warmup

    ### Methods

    `step(self) ‑> float`
    :

`Warmup(init: float, final: float, n_steps: int)`
:   Linearly increases from `init` to `final` over `n_warmup_steps` steps.

    ### Ancestors (in MRO)

    * saev.training.Scheduler

    ### Methods

    `step(self) ‑> float`
    :

Module saev.visuals
===================
There is some important notation used only in this file to dramatically shorten variable names.

Variables suffixed with `_im` refer to entire images, and variables suffixed with `_p` refer to patches.

Functions
---------

`batched_idx(total_size: int, batch_size: int) ‑> Iterator[tuple[int, int]]`
:   Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.
    
    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.
    
    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.

`dump_activations(cfg: saev.config.Visuals)`
:   For each SAE latent, we want to know which images have the most total "activation".
    That is, we keep track of each patch

`gather_batched(value: jaxtyping.Float[Tensor, 'batch n dim'], i: jaxtyping.Int[Tensor, 'batch k']) ‑> jaxtyping.Float[Tensor, 'batch k dim']`
:   

`get_new_topk(val1: jaxtyping.Float[Tensor, 'd_sae k'], i1: jaxtyping.Int[Tensor, 'd_sae k'], val2: jaxtyping.Float[Tensor, 'd_sae k'], i2: jaxtyping.Int[Tensor, 'd_sae k'], k: int) ‑> tuple[jaxtyping.Float[Tensor, 'd_sae k'], jaxtyping.Int[Tensor, 'd_sae k']]`
:   Picks out the new top k values among val1 and val2. Also keeps track of i1 and i2, then indices of the values in the original dataset.
    
    Args:
        val1: top k original SAE values.
        i1: the patch indices of those original top k values.
        val2: top k incoming SAE values.
        i2: the patch indices of those incoming top k values.
        k: k.
    
    Returns:
        The new top k values and their patch indices.

`get_sae_acts(vit_acts: jaxtyping.Float[Tensor, 'n d_vit'], sae: saev.nn.SparseAutoencoder, cfg: saev.config.Visuals) ‑> jaxtyping.Float[Tensor, 'n d_sae']`
:   Get SAE hidden layer activations for a batch of ViT activations.
    
    Args:
        vit_acts: Batch of ViT activations
        sae: Sparse autoencder.
        cfg: Experimental config.

`get_topk_img(cfg: saev.config.Visuals) ‑> saev.visuals.TopKImg`
:   Gets the top k images for each latent in the SAE.
    The top k images are for latent i are sorted by
    
        max over all images: f_x(cls)[i]
    
    Thus, we will never have duplicate images for a given latent.
    But we also will not have patch-level activations (a nice heatmap).
    
    Args:
        cfg: Config.
    
    Returns:
        A tuple of TopKImg and the first m features' activation distributions.

`get_topk_patch(cfg: saev.config.Visuals) ‑> saev.visuals.TopKPatch`
:   Gets the top k images for each latent in the SAE.
    The top k images are for latent i are sorted by
    
        max over all patches: f_x(patch)[i]
    
    Thus, we could end up with duplicate images in the top k, if an image has more than one patch that maximally activates an SAE latent.
    
    Args:
        cfg: Config.
    
    Returns:
        A tuple of TopKPatch and m randomly sampled activation distributions.

`main(cfg: saev.config.Visuals)`
:   .. todo:: document this function.
    
    Dump top-k images to a directory.
    
    Args:
        cfg: Configuration object.

`make_img(elem: saev.visuals.GridElement, *, upper: float | None = None) ‑> PIL.Image.Image`
:   

`plot_activation_distributions(cfg: saev.config.Visuals, distributions: jaxtyping.Float[Tensor, 'm n'])`
:   

`safe_load(path: str) ‑> object`
:   

`test_online_quantile_estimation(true: float, percentile: float)`
:   

Classes
-------

`GridElement(img: PIL.Image.Image, label: str, patches: jaxtyping.Float[Tensor, 'n_patches'])`
:   GridElement(img: PIL.Image.Image, label: str, patches: jaxtyping.Float[Tensor, 'n_patches'])

    ### Class variables

    `img: PIL.Image.Image`
    :

    `label: str`
    :

    `patches: jaxtyping.Float[Tensor, 'n_patches']`
    :

`PercentileEstimator(percentile: float | int, total: int, lr: float = 0.001, shape: tuple[int, ...] = ())`
:   

    ### Instance variables

    `estimate`
    :

    ### Methods

    `update(self, x)`
    :   Update the estimator with a new value.
        
        This method maintains the marker positions using the P2 algorithm rules.
        When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions.
        
        Arguments:
            x: The new value to incorporate into the estimation

`TopKImg(top_values: jaxtyping.Float[Tensor, 'd_sae k'], top_i: jaxtyping.Int[Tensor, 'd_sae k'], mean_values: jaxtyping.Float[Tensor, 'd_sae'], sparsity: jaxtyping.Float[Tensor, 'd_sae'], distributions: jaxtyping.Float[Tensor, 'm n'], percentiles: jaxtyping.Float[Tensor, 'd_sae'])`
:   .. todo:: Document this class.

    ### Class variables

    `distributions: jaxtyping.Float[Tensor, 'm n']`
    :

    `mean_values: jaxtyping.Float[Tensor, 'd_sae']`
    :

    `percentiles: jaxtyping.Float[Tensor, 'd_sae']`
    :

    `sparsity: jaxtyping.Float[Tensor, 'd_sae']`
    :

    `top_i: jaxtyping.Int[Tensor, 'd_sae k']`
    :

    `top_values: jaxtyping.Float[Tensor, 'd_sae k']`
    :

`TopKPatch(top_values: jaxtyping.Float[Tensor, 'd_sae k n_patches_per_img'], top_i: jaxtyping.Int[Tensor, 'd_sae k'], mean_values: jaxtyping.Float[Tensor, 'd_sae'], sparsity: jaxtyping.Float[Tensor, 'd_sae'], distributions: jaxtyping.Float[Tensor, 'm n'], percentiles: jaxtyping.Float[Tensor, 'd_sae'])`
:   .. todo:: Document this class.

    ### Class variables

    `distributions: jaxtyping.Float[Tensor, 'm n']`
    :

    `mean_values: jaxtyping.Float[Tensor, 'd_sae']`
    :

    `percentiles: jaxtyping.Float[Tensor, 'd_sae']`
    :

    `sparsity: jaxtyping.Float[Tensor, 'd_sae']`
    :

    `top_i: jaxtyping.Int[Tensor, 'd_sae k']`
    :

    `top_values: jaxtyping.Float[Tensor, 'd_sae k n_patches_per_img']`
    :

Module contrib
==============

Sub-modules
-----------
* contrib.classification
* contrib.semprobe
* contrib.semseg

Module contrib.classification
=============================
# Reproduce

You can reproduce our classification control experiments from our preprint by following these instructions.

The big overview (as described in our paper) is:

1. Train an SAE on the ImageNet-1K patch activations from a CLIP ViT-B/16, from the 11th (second-to-last) layer.
2. Show that you get meaningful features, through visualizations.
3. Train a linear probe on the [CLS] token activations from  a CLIP ViT-B/16, from the 12th layer, on the Caltech-101 dataset. We use an arbitrary random train/test split.
4. Show that we get good accuracy.
5. Manipulate the activations using the proposed SAE features.
6. Be amazed. :)

To do these steps:

## Record ImageNet-1K activations

## Train an SAE on Activations

```sh
uv run python -m saev train \
  --sweep configs/preprint/classification.toml \
  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8/ \
  --data.layer -2 \
  --sae.d-vit 768
```

## Visualize the SAE Features

`` was the best checkpoint from my sweep.

```sh
uv run python -m saev visuals \
  --ckpt checkpoints/bd97z80b/sae.pt \
  --dump-to /research/nfs_su_809/workspace/stevens.994/saev/features/bd97z80b \
  --sort-by patch \
  --data.shard-root /local/scratch/stevens.994/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8/ \
  --data.layer -2 \
  --log-freq-range -2.5 -1.5 \
  --log-value-range 0.0 1.0 \
  images:imagenet-dataset
```

You can see some neat features in here by using `saev.interactive.features` with `marimo`.

## Record CUB-200-2011 Activations

For each `$SPLIT` in "train" and "test":

```sh
uv run python -m saev activations \
  --vit-family clip \
  --vit-ckpt ViT-B-16/openai \
  --d-vit 768 \
  --n-patches-per-img 196 \
  --layers -2 -1 \
  --dump-to /local/scratch/$USER/cache/saev \
  --n-patches-per-shard 2_4000_000 \
  data:image-folder-dataset \
  --data.root /nfs/datasets/caltech-101/$SPLIT
```

## Train a Linear Probe

```sh
uv run python -m contrib.classification \
  --n-workers 32 \
  --train-imgs.root /research/nfs_su_809/workspace/stevens.994/datasets/cub2011/train \
  --val-imgs.root /research/nfs_su_809/workspace/stevens.994/datasets/cub2011/test/ \
  --sweep contrib/classification/sweep.toml
  ```

Then look at `logs/contrib/classification/hparam-sweeps.png`. 
It probably works for any of the learning rates above 1e-5 or so.

## Manipulate

Now we will manipulate the inputs to the probe by using the directions proposed by the SAE trained on ImageNet-1K and observe the changes in the linear model's predictions.
There are two ways to do this:

1. The marimo dashboard, which requires that you run your own inference.
2. The online dashboard, which is more polished but offers less control.

Since you have gone through the effort of training all this stuff, you probably want more control and have the hardware for inference.

Run the marimo dashboard with:

```sh
uv run marimo edit contrib/classification/interactive.py
```

These screenshots show the kinds of findings you can uncover with this dashboard.

First, when you open the dashboard and configure the options, you will eventually see something like this:

![Default dashbaord view of a sunflower example.](/assets/contrib/classification/sunflower-unchanged.png)

The main parts of the dashboard:

1. Example selector: choose which test image to classify. The image is shown on the bottom left.
2. The top SAE latents for the test image's class (in purple below). The latent values of $h$ are also shown. Many will be 0 because SAE latents fire very rarely (*sparse* autoencoder).
3. The top SAE latents for another, user-selected class (in orange below). Choose the class on the top right dropdown.
4. The top classes as predicted by the pre-trained classification model (a linear probe; shown in green below). 
5. The top classes as predicted by the *same* pre-trained classification model, *after* modifying the dense vector representation with the SAE's vectors. These predictions are updated as you change the sliders on the screen.

![Annotated dashbaord view of a sunflower example.](/saev/assets/contrib/classification/sunflower-unchanged-annotated.png)

As an example, you can scale *up* the top bonsai features. 
As you do, the most likely class will be a bonsai.
See below.

![A sunflower changed to look like a bonsai.](/saev/assets/contrib/classification/class-manipulation.png)

Here's another example.
With another sunflower, you can manipulate turn up the SAE feature that fires strongly on pagodas and other traditionally Asian architectural structures.
If you do, the most likley classification is a lotus, which is popular in Japanese and other Asian cultures.

![A sunflower changed to be a lotus (a culturally Asian flower).](/saev/assets/contrib/classification/japanese-culture.png)

Only once you turn up the SAE feature that fires strongly on potted plants does the classification change to bonsai (which are typically potted).

![A sunflower changed to "bonsai".](/saev/assets/contrib/classification/bonsai.png)

I encourage you to look at other test images and manipulate the predictions!

## Make Figures

```sh
uv run scripts/preprint/make_figures.py classification \
  --probs-before "Blue Jay" 0.49 "Clark\nNutcracker" 0.15 "White-Breasted\nNuthatch" 0.11 "Florida\nJay" 0.07 \
  --probs-after "Clark\nNutcracker" 0.31 "White-Breasted\nNuthatch" 0.19 "Great Grey\nShrike" 0.11 "Blue Jay" 0.10
```

Sub-modules
-----------
* contrib.classification.config
* contrib.classification.download
* contrib.classification.training
* contrib.classification.transforms

Module contrib.classification.config
====================================

Functions
---------

`grid(cfg: contrib.classification.config.Train, sweep_dct: dict[str, object]) ‑> tuple[list[contrib.classification.config.Train], list[str]]`
:   

Classes
-------

`Train(learning_rate: float = 0.0001, weight_decay: float = 0.001, n_epochs: int = 20, batch_size: int = 512, n_workers: int = 32, train_imgs: saev.config.ImageFolderDataset = <factory>, val_imgs: saev.config.ImageFolderDataset = <factory>, device: str = 'cuda', ckpt_path: str = './checkpoints/contrib/classification', seed: int = 42, log_to: str = './logs/contrib/classification')`
:   Train(learning_rate: float = 0.0001, weight_decay: float = 0.001, n_epochs: int = 20, batch_size: int = 512, n_workers: int = 32, train_imgs: saev.config.ImageFolderDataset = <factory>, val_imgs: saev.config.ImageFolderDataset = <factory>, device: str = 'cuda', ckpt_path: str = './checkpoints/contrib/classification', seed: int = 42, log_to: str = './logs/contrib/classification')

    ### Class variables

    `batch_size: int`
    :   Training batch size.

    `ckpt_path: str`
    :

    `device: str`
    :   Hardware to train on.

    `learning_rate: float`
    :   Linear layer learning rate.

    `log_to: str`
    :

    `n_epochs: int`
    :   Number of training epochs for linear layer.

    `n_workers: int`
    :   Number of dataloader workers.

    `seed: int`
    :   Random seed.

    `train_imgs: saev.config.ImageFolderDataset`
    :   Configuration for the training images.

    `val_imgs: saev.config.ImageFolderDataset`
    :   Configuration for the validation images.

    `weight_decay: float`
    :   Weight decay  for AdamW.

Namespace contrib.classification.download
=========================================

Sub-modules
-----------
* contrib.classification.download.download_caltech101
* contrib.classification.download.download_cub
* contrib.classification.download.download_flowers

Module contrib.classification.download.download_caltech101
==========================================================
A script to download the Caltech101 dataset for use as an saev.activations.ImageFolderDataset.

```sh
uv run contrib/classification/download_flowers.py --help
```

Functions
---------

`main(args: contrib.classification.download.download_caltech101.Args)`
:   Download Caltech 101.

Classes
-------

`Args(dir: str = '.', chunk_size_kb: int = 1, seed: int = 42)`
:   Configure download options.

    ### Class variables

    `chunk_size_kb: int`
    :   How many KB to download at a time before writing to file.

    `dir: str`
    :   Where to save data.

    `seed: int`
    :   Random seed used to generate split.

Module contrib.classification.download.download_cub
===================================================

Functions
---------

`main(args: contrib.classification.download.download_cub.Args)`
:   Download CUB-200-2011.

Classes
-------

`Args(dir: str = '.', chunk_size_kb: int = 1, seed: int = 42, download: bool = True, extract: bool = True)`
:   Configure download options.

    ### Class variables

    `chunk_size_kb: int`
    :   How many KB to download at a time before writing to file.

    `dir: str`
    :   Where to save data.

    `download: bool`
    :   Whether to download.

    `extract: bool`
    :   Whether to extract from .tgz file.

    `seed: int`
    :   Random seed used to generate split.

Module contrib.classification.download.download_flowers
=======================================================
A script to download the Flowers102 dataset.

```sh
uv run contrib/classification/download_flowers.py --help
```

Functions
---------

`main(args: contrib.classification.download.download_flowers.Args)`
:   Download NeWT.

Classes
-------

`Args(dir: str = '.', chunk_size_kb: int = 1)`
:   Configure download options.

    ### Class variables

    `chunk_size_kb: int`
    :   How many KB to download at a time before writing to file.

    `dir: str`
    :   Where to save data.

Module contrib.classification.training
======================================
Train a linear probe on [CLS] activations from a ViT.

Functions
---------

`check_cfgs(cfgs: list[contrib.classification.config.Train])`
:   

`dump_model(cfg: contrib.classification.config.Train, model: torch.nn.modules.module.Module)`
:   Save a model checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).

`get_dataloader(cfg: contrib.classification.config.Train, *, is_train: bool)`
:   

`load_acts(cfg: saev.config.DataLoad) ‑> jaxtyping.Float[Tensor, 'n d_vit']`
:   

`load_class_headers(cfg: saev.config.ImageFolderDataset) ‑> list[str]`
:   

`load_model(fpath: str, *, device: str = 'cpu') ‑> torch.nn.modules.module.Module`
:   Loads a linear layer from disk.

`load_targets(cfg: saev.config.ImageFolderDataset) ‑> jaxtyping.Int[Tensor, 'n']`
:   

`main(cfgs: list[contrib.classification.config.Train])`
:   

`make_models(cfgs: list[contrib.classification.config.Train], d_out: int) ‑> tuple[torch.nn.modules.container.ModuleList, list[dict[str, object]]]`
:   

Classes
-------

`Dataset(acts_cfg: saev.config.DataLoad, imgs_cfg: saev.config.ImageFolderDataset)`
:   An abstract class representing a :class:`Dataset`.
    
    All datasets that represent a map from keys to data samples should subclass
    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
    data sample for a given key. Subclasses could also optionally overwrite
    :meth:`__len__`, which is expected to return the size of the dataset by many
    :class:`~torch.utils.data.Sampler` implementations and the default options
    of :class:`~torch.utils.data.DataLoader`. Subclasses could also
    optionally implement :meth:`__getitems__`, for speedup batched samples
    loading. This method accepts list of indices of samples of batch and returns
    list of samples.
    
    .. note::
      :class:`~torch.utils.data.DataLoader` by default constructs an index
      sampler that yields integral indices.  To make it work with a map-style
      dataset with non-integral indices/keys, a custom sampler must be provided.

    ### Ancestors (in MRO)

    * torch.utils.data.dataset.Dataset
    * typing.Generic

    ### Instance variables

    `d_vit: int`
    :

    `n_classes: int`
    :

Module contrib.classification.transforms
========================================
Contains the transforms used in every spot:

* Training
* Making figures
* Web app

For both figures and the webapp, the transform is:

1. Resize the image so that the shortest size is 512 pixels.
2. Take the middle 448x448 as a crop

This gives us object-centric images that are 448x448 (fixed pixel size is important for the web app) and not distorted.

Functions
---------

`for_figures()`
:   

`for_training(vit_ckpt: str)`
:   

`for_webapp(img: PIL.Image.Image) ‑> PIL.Image.Image`
:

Namespace contrib.semprobe
==========================

Sub-modules
-----------
* contrib.semprobe.config

Module contrib.semprobe.config
==============================

Classes
-------

`Negatives(dump_to: str = './data/contrib/semprobe/test', imgs: saev.config.ImagenetDataset | saev.config.ImageFolderDataset | saev.config.Ade20kDataset = <factory>, classes: list[str] = <factory>, n_imgs: int = 20, skip: list[str] = <factory>, seed: int = 42)`
:   Negatives(dump_to: str = './data/contrib/semprobe/test', imgs: saev.config.ImagenetDataset | saev.config.ImageFolderDataset | saev.config.Ade20kDataset = <factory>, classes: list[str] = <factory>, n_imgs: int = 20, skip: list[str] = <factory>, seed: int = 42)

    ### Class variables

    `classes: list[str]`
    :   Which classes to randomly sample.

    `dump_to: str`
    :   Where to save negative samples.

    `imgs: saev.config.ImagenetDataset | saev.config.ImageFolderDataset | saev.config.Ade20kDataset`
    :   Where to sample images from.

    `n_imgs: int`
    :   Number of negative images.

    `seed: int`
    :   Random seed.

    `skip: list[str]`
    :   Which images to skip.

`Score(sae_ckpt: str = './checkpoints/abcdefg/sae.pt', batch_size: int = 2048, n_workers: int = 32, thresholds: list[float] = <factory>, top_k: int = 5, imgs: saev.config.ImageFolderDataset = <factory>, acts: saev.config.DataLoad = <factory>, dump_to: str = './logs/contrib/semprobe', include_latents: list[int] = <factory>, device: str = 'cuda')`
:   Score(sae_ckpt: str = './checkpoints/abcdefg/sae.pt', batch_size: int = 2048, n_workers: int = 32, thresholds: list[float] = <factory>, top_k: int = 5, imgs: saev.config.ImageFolderDataset = <factory>, acts: saev.config.DataLoad = <factory>, dump_to: str = './logs/contrib/semprobe', include_latents: list[int] = <factory>, device: str = 'cuda')

    ### Class variables

    `acts: saev.config.DataLoad`
    :   SAE activations for the curated examples.

    `batch_size: int`
    :   Batch size for SAE inference.

    `device: str`
    :   Hardware device.

    `dump_to: str`
    :   Where to save results/visualizations.

    `imgs: saev.config.ImageFolderDataset`
    :   Where curated examples are stored

    `include_latents: list[int]`
    :   Latents to manually include.

    `n_workers: int`
    :   Number of dataloader workers.

    `sae_ckpt: str`
    :   Path to SAE checkpoint

    `thresholds: list[float]`
    :   Threshold(s) for feature activation.

    `top_k: int`
    :   Number of top features to manually analyze.

Module contrib.semseg
=====================
Interpret and manipulate semantic segmentation models using SAEs.

# Reproduce

There are two main experiments to reproduce in our preprint.

First, our qualitative examples.
Second, our quantitative evaluation of pseudo-orthogonality.

## Qualitative

You can reproduce our qualititative examples from our preprint by following these instructions.

1. Train a linear probe on semantic segmentation task using ADE20K.
2. Measure linear probe baseline metrics.
3. Manipulate the activations using the proposed SAE features.
4. Be amazed. :)

Details can be found below.

### Train a Linear Probe on Semantic Segmentation

Train a linear probe on DINOv2 activations from ADE20K.
It's fixed with DINOv2 because of patch size, but the code could be extended to different ViTs.

```sh
uv run python -m contrib.semseg train \
  --sweep contrib/semseg/sweep.toml \
  --imgs.root /$NFS/$USER/datasets/ade20k
```

### Measure Linear Probe Baseline Metrics

Check which learning rate/weight decay combination is best for the linear probe.

```sh
uv run python -m contrib.semseg validate \
  --imgs.root /$NFS/$USER/datasets/ade20k
```

Then you can look in `./logs/contrib/semseg` for `hparam-sweeps.png` to see what learning rate/weight decay combination is best.

### Manipulate the Activations

You need an SAE that's been trained on DINOv2's activations on ImageNet.
Then you can run both the frontend server and the backend server:

**Frontend:**

```sh
uv run python -m http.server
```

Then navigate to [http://localhost:8000/web/apps/semseg/](http://localhost:8000/web/apps/semseg/).

**Backend:**

This is a little trickier because the backend server lives on Huggingface spaces and talks to a personal Cloudflare server.

[TODO]

## Quantitative

We aim to measure the specificity and psuedo-orthogonality of SAE-discovered features by evaluating the impact of feature manipulation on semantic segmentation.

We train an SAE on ImageNet-1K activations from DINOv2 ViT-B/14 ([hosted here on HuggingFace](https://huggingface.co/osunlp/SAE_DINOv2_24K_ViT-B-14_IN1K)).
Then, we train a linear probe on top of DINOv2 for ADE20K following the procedure above.
We define four ways to select a feature vector for a given ADE20K class:

1. Random unit vector in $d$-dimensional space
2. Random SAE feature vector.
3. Automatically selected SAE feature vector.
4. Manually chosen SAE feature vector.

All four are described in more detail below.

Given a feature $i$ and an ADE20K class $c$, for each image in the validation set, we perform semantic segmentation inference using DINOv2 and the trained linear probe.
However, we set feature $i$ to $-2$ its maximum observed value following the description of manipulation in Section 3.3 of our preprint.
We then maintain several counts:

1. Number of patches originally predicted as class $c$ and are now *not* $c$.
2. Number of patches originally predicted as class $c$ and are now *still* $c$.
3. Number of patches originally predicted as *not* class $c$ and are now $c$.
4. Number of patches originally predicted as *not* class $c$ and are now *still not* $c$.

With this, we calculate two percentages:

1. Target change rate: `(Number of original $c$ patches that changed class) / (Total number of original $c$ patches) * 100`
2. Other change rate: `(Number of original not-$c$ patches that changed class) / (Total number of original not-$c$ patches) * 100`

Ideally, we maximize target change rate and minimize other change rate.
We measure mean target change rate across all classes and mean other change rate across all classes.

```sh
uv run python -m contrib.semseg quantify \
  --sae-ckpt checkpoints/public/oebd6e6i/sae.pt \
  --seg-ckpt checkpoints/contrib/semseg/lr_0_001__wd_0_1/ \
  --imgs.root /$NFS/$USER/datasets/ade20k/
```

The main entry point is `contrib/semseg/__main__.py`.
Run `uv run python -m contrib.semseg --help` to see all options.

Sub-modules
-----------
* contrib.semseg.config
* contrib.semseg.interactive
* contrib.semseg.quantitative
* contrib.semseg.training
* contrib.semseg.validation
* contrib.semseg.visuals

Module contrib.semseg.config
============================
Configs for all the different subscripts in `contrib.semseg`.

Imports must be fast in this file, as described in `saev.config`.
So do not import torch, numpy, etc.

Functions
---------

`grid(cfg: contrib.semseg.config.Train, sweep_dct: dict[str, object]) ‑> tuple[list[contrib.semseg.config.Train], list[str]]`
:   

Classes
-------

`Quantitative(sae_ckpt: str = './checkpoints/sae.pt', seg_ckpt: str = './checkpoints/contrib/semseg/best.pt', top_values: str = './data/sort_by_patch/top_values.pt', sparsity: str = './data/sort_by_patch/sparsity.pt', act_mean: str = './data/contrib/semseg/dinov2_imagenet1k_mean.pt', act_norm: float = 2.0181241035461426, label_threshold: float = 0.9, vit_family: Literal['clip', 'siglip', 'dinov2', 'moondream2'] = 'dinov2', vit_ckpt: str = 'dinov2_vitb14_reg', vit_layer: int = 11, patch_size_px: tuple[int, int] = (14, 14), n_patches_per_img: int = 256, cls_token: bool = True, imgs: saev.config.Ade20kDataset = <factory>, batch_size: int = 128, n_workers: int = 32, scale: float = -2.0, top_k: int = 3, device: str = 'cuda', dump_to: str = './logs/contrib/semseg/quantitative', seed: int = 42)`
:   Quantitative(sae_ckpt: str = './checkpoints/sae.pt', seg_ckpt: str = './checkpoints/contrib/semseg/best.pt', top_values: str = './data/sort_by_patch/top_values.pt', sparsity: str = './data/sort_by_patch/sparsity.pt', act_mean: str = './data/contrib/semseg/dinov2_imagenet1k_mean.pt', act_norm: float = 2.0181241035461426, label_threshold: float = 0.9, vit_family: Literal['clip', 'siglip', 'dinov2', 'moondream2'] = 'dinov2', vit_ckpt: str = 'dinov2_vitb14_reg', vit_layer: int = 11, patch_size_px: tuple[int, int] = (14, 14), n_patches_per_img: int = 256, cls_token: bool = True, imgs: saev.config.Ade20kDataset = <factory>, batch_size: int = 128, n_workers: int = 32, scale: float = -2.0, top_k: int = 3, device: str = 'cuda', dump_to: str = './logs/contrib/semseg/quantitative', seed: int = 42)

    ### Class variables

    `act_mean: str`
    :   Where to load activation mean from.

    `act_norm: float`
    :   How much to scale activations such that average dataset norm is approximately sqrt(d_vit).

    `batch_size: int`
    :   Batch size for inference.

    `cls_token: bool`
    :   Whether the model has a [CLS] token.

    `device: str`
    :   Hardware for inference.

    `dump_to: str`
    :   Directory to save results to.

    `imgs: saev.config.Ade20kDataset`
    :   Data configuration for ADE20K dataset.

    `label_threshold: float`
    :   Proportion of pixels that must have the same label to consider a given patch when calculating F1.

    `max_freq`
    :   Maximum frequency. Any feature that fires more than this is ignored.

    `n_patches_per_img: int`
    :   Number of ViT patches per image (depends on model).

    `n_workers: int`
    :   Number of dataloader workers.

    `patch_size_px: tuple[int, int]`
    :   ViT patch size.

    `sae_ckpt: str`
    :   Path to trained SAE checkpoint.

    `scale: float`
    :   Intervention scale. Likely needs to be larger for random-vector.

    `seed: int`
    :   Random seed.

    `seg_ckpt: str`
    :   Path to trained segmentation head.

    `sparsity: str`
    :   Path to sparsity.pt file generated by `saev visuals`.

    `top_k: int`
    :   Number of latents to show.

    `top_values: str`
    :   Path to top_values.pt file generated by `saev visuals`.

    `vit_ckpt: str`
    :   Specific ViT checkpoint.

    `vit_family: Literal['clip', 'siglip', 'dinov2', 'moondream2']`
    :   Which ViT family.

    `vit_layer: int`
    :   Vit layer to read/modify.

`Train(learning_rate: float = 0.0001, weight_decay: float = 0.001, n_epochs: int = 400, batch_size: int = 1024, n_workers: int = 32, imgs: saev.config.Ade20kDataset = <factory>, eval_every: int = 100, device: str = 'cuda', ckpt_path: str = './checkpoints/contrib/semseg', seed: int = 42, log_to: str = './logs/contrib/semseg')`
:   Train(learning_rate: float = 0.0001, weight_decay: float = 0.001, n_epochs: int = 400, batch_size: int = 1024, n_workers: int = 32, imgs: saev.config.Ade20kDataset = <factory>, eval_every: int = 100, device: str = 'cuda', ckpt_path: str = './checkpoints/contrib/semseg', seed: int = 42, log_to: str = './logs/contrib/semseg')

    ### Class variables

    `batch_size: int`
    :   Training batch size for linear layer.

    `ckpt_path: str`
    :

    `device: str`
    :   Hardware to train on.

    `eval_every: int`
    :   How many epochs between evaluations.

    `imgs: saev.config.Ade20kDataset`
    :   Configuration for the ADE20K dataset.

    `learning_rate: float`
    :   Linear layer learning rate.

    `log_to: str`
    :

    `n_epochs: int`
    :   Number of training epochs for linear layer.

    `n_workers: int`
    :   Number of dataloader workers.

    `seed: int`
    :   Random seed.

    `weight_decay: float`
    :   Weight decay  for AdamW.

`Validation(ckpt_root: str = './checkpoints/contrib/semseg', dump_to: str = './logs/contrib/semseg', imgs: saev.config.Ade20kDataset = <factory>, batch_size: int = 128, n_workers: int = 32, device: str = 'cuda')`
:   Validation(ckpt_root: str = './checkpoints/contrib/semseg', dump_to: str = './logs/contrib/semseg', imgs: saev.config.Ade20kDataset = <factory>, batch_size: int = 128, n_workers: int = 32, device: str = 'cuda')

    ### Class variables

    `batch_size: int`
    :   Batch size for calculating F1 scores.

    `ckpt_root: str`
    :   Root to all checkpoints to evaluate.

    `device: str`
    :   Hardware for linear probe inference.

    `dump_to: str`
    :   Directory to dump results to.

    `imgs: saev.config.Ade20kDataset`
    :   Configuration for the ADE20K validation dataset.

    `n_workers: int`
    :   Number of dataloader workers.

`Visuals(sae_ckpt: str = './checkpoints/sae.pt', ade20k_cls: int = 29, k: int = 32, acts: saev.config.DataLoad = <factory>, imgs: saev.config.Ade20kDataset = <factory>, batch_size: int = 128, n_workers: int = 32, label_threshold: float = 0.9, device: str = 'cuda')`
:   Visuals(sae_ckpt: str = './checkpoints/sae.pt', ade20k_cls: int = 29, k: int = 32, acts: saev.config.DataLoad = <factory>, imgs: saev.config.Ade20kDataset = <factory>, batch_size: int = 128, n_workers: int = 32, label_threshold: float = 0.9, device: str = 'cuda')

    ### Class variables

    `acts: saev.config.DataLoad`
    :   Configuration for the saved ADE20K training ViT activations.

    `ade20k_cls: int`
    :   ADE20K class to probe for.

    `batch_size: int`
    :   Batch size for calculating F1 scores.

    `device: str`
    :   Hardware for SAE inference.

    `imgs: saev.config.Ade20kDataset`
    :   Configuration for the ADE20K training dataset.

    `k: int`
    :   Top K features to save.

    `label_threshold: float`
    :

    `n_workers: int`
    :   Number of dataloader workers.

    `sae_ckpt: str`
    :   Path to the sae.pt file.

Module contrib.semseg.interactive
=================================

Module contrib.semseg.quantitative
==================================

Functions
---------

`argmax_logits(logits_BPC: jaxtyping.Float[Tensor, 'batch patches channels_with_null']) ‑> jaxtyping.Int[Tensor, 'batch patches']`
:   

`compute_class_results(orig_preds: jaxtyping.Int[Tensor, 'n_imgs patches'], mod_preds: jaxtyping.Int[Tensor, 'n_imgs patches']) ‑> list[contrib.semseg.quantitative.ClassResults]`
:   

`eval_auto_feat(cfg: contrib.semseg.config.Quantitative, sae: saev.nn.SparseAutoencoder, clf: torch.nn.modules.module.Module, dataloader) ‑> contrib.semseg.quantitative.Report`
:   

`eval_rand_feat(cfg: contrib.semseg.config.Quantitative, sae: saev.nn.SparseAutoencoder, clf: torch.nn.modules.module.Module, dataloader) ‑> contrib.semseg.quantitative.Report`
:   Evaluates the effects of suppressing a random SAE feature.
    
    Args:
        cfg: Configuration for quantitative evaluation
        sae: Trained sparse autoencoder model
        clf: Trained classifier model
        dataloader: DataLoader providing batches of images
    
    Returns:
        Report containing intervention results, including per-class changes

`eval_rand_vec(cfg: contrib.semseg.config.Quantitative, sae: saev.nn.SparseAutoencoder, clf: torch.nn.modules.module.Module, dataloader) ‑> contrib.semseg.quantitative.Report`
:   Evaluates the effects of adding a random unit vector to the patches.
    
    Args:
        cfg: Configuration for quantitative evaluation
        sae: Trained sparse autoencoder model
        clf: Trained classifier model
        dataloader: DataLoader providing batches of images
    
    Returns:
        Report containing intervention results, including per-class changes

`get_latent_lookup(cfg: contrib.semseg.config.Quantitative, sae: saev.nn.SparseAutoencoder, dataloader) ‑> jaxtyping.Int[Tensor, '151']`
:   Dimension key:
    
    * B: batch dimension
    * P: patches per image
    * D: ViT hidden dimension
    * S: SAE feature dimension
    * T: threshold dimension
    * C: class dimension
    * L: layer dimension

`get_patch_i(i: jaxtyping.Int[Tensor, 'batch width height'], n_patches_per_img: int) ‑> jaxtyping.Int[Tensor, 'batch width height']`
:   

`get_patch_mask(pixel_labels_NP: jaxtyping.UInt8[Tensor, 'n patch_px'], threshold: float) ‑> jaxtyping.Bool[Tensor, 'n']`
:   Create a mask for patches where at least threshold proportion of pixels have the same label.
    
    Args:
        pixel_labels_NP: Tensor of shape [n, patch_pixels] with pixel labels
        threshold: Minimum proportion of pixels with same label
    
    Returns:
        Tensor of shape [n] with True for patches that pass the threshold

`main(cfg: contrib.semseg.config.Quantitative)`
:   Main entry point for quantitative evaluation.

`map_range(x: jaxtyping.Float[Tensor, '*batch'], domain: tuple[float | int, float | int], range: tuple[float | int, float | int]) ‑> jaxtyping.Float[Tensor, '*batch']`
:   

`register_hook(vit: torch.nn.modules.module.Module, hook: Callable[[jaxtyping.Float[Tensor, '...']], jaxtyping.Float[Tensor, '...']], layer: int, n_patches_per_img: int)`
:   

`save(results: list[contrib.semseg.quantitative.Report], fpath: str) ‑> None`
:   Save evaluation results to a CSV file.
    
    Args:
        results: List of Report objects containing evaluation results
        dpath: Path to save the CSV file

`unscaled(x: jaxtyping.Float[Tensor, '*batch'], max_obs: float | int) ‑> jaxtyping.Float[Tensor, '*batch']`
:   Scale from [-10, 10] to [10 * -max_obs, 10 * max_obs].

Classes
-------

`ClassResults(class_id: int, class_name: str, n_orig_patches: int, n_changed_patches: int, n_other_patches: int, n_other_changed: int, change_distribution: dict[int, int])`
:   Results for a single class.

    ### Class variables

    `change_distribution: dict[int, int]`
    :   What classes did patches change to? Tracks how many times <value> a patch changed from self.class_id to <key>.

    `class_id: int`
    :   Numeric identifier for the class.

    `class_name: str`
    :   Human-readable name of the class.

    `n_changed_patches: int`
    :   After intervention, how many patches changed.

    `n_orig_patches: int`
    :   Original patches that were this class.

    `n_other_changed: int`
    :   After intervention, how many of the other patches changed.

    `n_other_patches: int`
    :   Total patches that weren't this class.

`Report(method: str, class_results: list[contrib.semseg.quantitative.ClassResults], intervention_scale: float)`
:   Complete results from an intervention experiment.

    ### Class variables

    `class_results: list[contrib.semseg.quantitative.ClassResults]`
    :   Per-class detailed results.

    `intervention_scale: float`
    :   Magnitude of intervention.

    `method: str`
    :   Which intervention method was used.

    ### Instance variables

    `mean_other_change: float`
    :   Percentage of non-target patches that changed class.

    `mean_target_change: float`
    :   Percentage of target patches that changed class.

    `other_change_std: float`
    :   Standard deviation of non-target patch changes across classes.

    `target_change_std: float`
    :   Standard deviation of change percentage across classes.

    ### Methods

    `to_csv_row(self) ‑> dict[str, float]`
    :   Convert to a row for the summary CSV.

Module contrib.semseg.training
==============================
Trains multiple linear probes in parallel on DINOv2's ADE20K activations.

Functions
---------

`batched_idx(total_size: int, batch_size: int) ‑> Iterator[tuple[int, int]]`
:   Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.
    
    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.
    
    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.

`batched_upsample_and_pred(tensor: jaxtyping.Float[Tensor, 'n channels width height'], *, size: tuple[int, int], mode: str, batch_size: int = 128) ‑> jaxtyping.Int[Tensor, 'n {size[0]} {size[1]}']`
:   

`check_cfgs(cfgs: list[contrib.semseg.config.Train])`
:   

`count_patches(ade20k: saev.config.Ade20kDataset, patch_size_px: tuple[int, int] = (14, 14), threshold: float = 0.9, n_workers: int = 8)`
:   Count the number of patches in the data that meets

`dump(cfg: contrib.semseg.config.Train, model: torch.nn.modules.module.Module, *, step: int | None = None)`
:   Save a model checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).

`get_class_ious(y_pred: jaxtyping.Int[Tensor, 'models batch width height'], y_true: jaxtyping.Int[Tensor, 'models batch width height'], n_classes: int, ignore_class: int | None = 0) ‑> jaxtyping.Float[Tensor, 'models n_classes']`
:   Calculate mean IoU for predicted masks.
    
    Arguments:
        y_pred:
        y_true:
        n_classes: Number of classes.
    
    Returns:
        Mean IoU as a float tensor.

`get_dataloader(cfg: contrib.semseg.config.Train, *, is_train: bool)`
:   

`load(fpath: str, *, device: str = 'cpu') ‑> torch.nn.modules.module.Module`
:   Loads a sparse autoencoder from disk.

`load_latest(dpath: str, *, device: str = 'cpu') ‑> torch.nn.modules.module.Module`
:   Loads the latest checkpoint by picking out the checkpoint file in dpath with the largest _step{step} suffix.
    
    Arguments:
        dpath: Directory to search.
        device: optional torch device to pass to load.

`main(cfgs: list[contrib.semseg.config.Train])`
:   

`make_models(cfgs: list[contrib.semseg.config.Train], d_vit: int) ‑> tuple[torch.nn.modules.container.ModuleList, list[dict[str, object]]]`
:   

Classes
-------

`Dataset(imgs_cfg: saev.config.Ade20kDataset)`
:   An abstract class representing a :class:`Dataset`.
    
    All datasets that represent a map from keys to data samples should subclass
    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
    data sample for a given key. Subclasses could also optionally overwrite
    :meth:`__len__`, which is expected to return the size of the dataset by many
    :class:`~torch.utils.data.Sampler` implementations and the default options
    of :class:`~torch.utils.data.DataLoader`. Subclasses could also
    optionally implement :meth:`__getitems__`, for speedup batched samples
    loading. This method accepts list of indices of samples of batch and returns
    list of samples.
    
    .. note::
      :class:`~torch.utils.data.DataLoader` by default constructs an index
      sampler that yields integral indices.  To make it work with a map-style
      dataset with non-integral indices/keys, a custom sampler must be provided.

    ### Ancestors (in MRO)

    * torch.utils.data.dataset.Dataset
    * typing.Generic

`DinoV2()`
:   Base class for all neural network modules.
    
    Your models should also subclass this class.
    
    Modules can also contain other Modules, allowing to nest them in
    a tree structure. You can assign the submodules as regular attributes::
    
        import torch.nn as nn
        import torch.nn.functional as F
    
        class Model(nn.Module):
            def __init__(self) -> None:
                super().__init__()
                self.conv1 = nn.Conv2d(1, 20, 5)
                self.conv2 = nn.Conv2d(20, 20, 5)
    
            def forward(self, x):
                x = F.relu(self.conv1(x))
                return F.relu(self.conv2(x))
    
    Submodules assigned in this way will be registered, and will have their
    parameters converted too when you call :meth:`to`, etc.
    
    .. note::
        As per the example above, an ``__init__()`` call to the parent class
        must be made before assignment on the child.
    
    :ivar training: Boolean represents whether this module is in training or
                    evaluation mode.
    :vartype training: bool
    
    Initialize internal Module state, shared by both nn.Module and ScriptModule.

    ### Ancestors (in MRO)

    * torch.nn.modules.module.Module

    ### Methods

    `forward(self, batch: jaxtyping.Float[Tensor, 'batch 3 width height']) ‑> Callable[..., Any]`
    :   Define the computation performed at every call.
        
        Should be overridden by all subclasses.
        
        .. note::
            Although the recipe for forward pass needs to be defined within
            this function, one should call the :class:`Module` instance afterwards
            instead of this since the former takes care of running the
            registered hooks while the latter silently ignores them.

Module contrib.semseg.validation
================================
Checks which checkpoints have the best validation loss, mean IoU, class-specific IoU, validation accuracy, and qualitative results.

Writes results to CSV files and hparam graphs (in-progress).

Functions
---------

`load_ckpts(root: str, *, device: str = 'cpu') ‑> list[tuple[contrib.semseg.config.Train, torch.nn.modules.module.Module]]`
:   Loads the latest checkpoints for each directory within root.
    
    Arguments:
        root: directory containing other directories with cfg.json and model_step{step}.pt files.
        device: where to load models.
    
    Returns:
        List of cfg, model pairs.

`main(cfg: contrib.semseg.config.Validation)`
:

Module contrib.semseg.visuals
=============================
Propose features for manual verification.

Functions
---------

`axis_unique(a: jaxtyping.Shaped[ndarray, '*axes'], axis: int = -1, return_counts: bool = True, *, null_value: int = -1) ‑> jaxtyping.Shaped[ndarray, '*axes'] | tuple[jaxtyping.Shaped[ndarray, '*axes'], jaxtyping.Int[ndarray, '*axes']]`
:   Calculate unique values and their counts along any axis of a matrix.
    
    Arguments:
        a: Input array
        axis: The axis along which to find unique values.
        return_counts: If true, also return the count of each unique value
    
    Returns:
        unique: Array of unique values, with zeros replacing duplicates
        counts: (optional) Count of each unique value (only if return_counts=True)

`main(cfg: contrib.semseg.config.Visuals)`
: