<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <title>Sparse Autoencoders for Vision</title>
  <meta name="description" content="Docs for saev.">
  <link rel="stylesheet" href="main.css"/>
</head>
<body>
  <main>
    <div style="text-align: center">
      <details>
        <summary>More Research</summary>
        <div class="options">
          <p><a href="https://imageomics.github.io/bioclip/">BioCLIP</a></p>
          <p><a href="https://github.com/Imageomics/INTR">INTR</a></p>
        </div>
      </details>
    </div>
    <article id="content">
    <h1>Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models</h1>
    <p class="centered">
      <a href="https://samuelstevens.me/">Samuel Stevens</a>,
      <a href="https://sites.google.com/view/wei-lun-harry-chao">Wei-Lun (Harry) Chao</a>,
      <a href="https://cse.osu.edu/people/berger-wolf.1">Tanya Berger-Wolf</a>,
      <a href="https://ysu1989.github.io/">Yu Su</a>
    </p>
    <p class="centered">
      The Ohio State University
    </p>
    <p class="text-sm centered">
      <a href="mailto:stevens.994@buckeyemail.osu.edu">stevens.994@osu.edu</a>
    </p>
    <p class="centered">
      <a class="pill-button" href="https://github.com/osu-nlp-group/SAE-V">
        <img src="assets/icons/github.svg" /> Code
      </a>
      <!-- <a class="pill-button" href="https://huggingface.co/osunlp/SAE-V"> -->
      <a class="pill-button" href="">
        <img src="assets/icons/huggingface.svg" /> Models <i>[coming soon]</i>
      </a>
      <a class="pill-button" href="#demos">
        <img src="assets/icons/internet.svg" /> Demos
      </a>
      <a class="pill-button" href="saev">
        <img src="assets/icons/internet.svg" /> API Docs
      </a>
      <a class="pill-button" href="https://arxiv.org/abs/2502.06755">
        <img src="assets/icons/arxiv.svg" /> Preprint
      </a>
    </p>
     <p id="abstract">
      To understand vision models, we cannot only interpret their features; we must also validate these interpretations through controlled experiments.
      Current approaches either provide interpretable features without the ability to test their causal influence, or provide uninterpretable model control.
      We apply sparse autoencoders (SAEs) to state-of-the-art vision models and reveal key differences in model-learned abstractions.
      We also show that SAEs can reliably identify and manipulate interpretable visual features without model re-training, providing a powerful tool for understanding and controlling vision model behavior.
    </p>
    <figure>
      <a href="assets/overview2.jpg"><img src="assets/overview2.webp" alt="" loading="lazy"></a>
      <figcaption>
      Sparse autoencoders (SAEs) trained on pre-trained ViT activations discover a wide spread of features across both visual patterns and semantic structures. We show eight different features from an SAE trained on ImageNet-1K activations from a CLIP-trained ViT-B/16.
      </figcaption>
    </figure>
    <h2 id="demos">Demos</h2>
    <p>
      There are two web-based demos to replicate the core findings in Section 5 in our <a href="https://arxiv.org/abs/2502.06755">preprint</a>.
    </p>
    <ol>
      <li><a href="demos/classification/?example=680">Interpreting bird classifications</a> (Section 5.1)</li>
      <li><a href="demos/semseg">Interpreting semantic segmentation</a> (Section 5.2)</li>
    </ol>
    <h2><code>saev</code></h2>
    <p>
      <code>saev</code> is a package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.
      It also includes some interactive demos for scientifically rigorous interpretation of ViTs.
    </p>
    <p>
      API reference docs are available below, as well as the <a href="https://github.com/osu-nlp-group/saev">source code on GitHub</a>.
    </p>
    <h2>API Docs</h2>
    <p>
      We provide auto-generated API docs based on docstrings.
      There is also a <a href="saev/#guide-to-training-saes-on-vision-models">guide</a> to getting started.
      You can also copy-paste all of the package from <a href="llms.txt">llms.txt</a> into a long-context model and ask it questions.
    <ol>
      <li><a href="saev">saev</a>: A package to train SAEs for vision transformers in PyTorch.</li>
      <li><a href="contrib">contrib</a>: Sub-packages for doing interesting things with the trained SAEs.</li>
    </ol>
    <h2>References &amp; Citations</h2>
    <p>
      Please cite our preprint or our code, depending on which is most valuable to your work.
    </p>
    <p>
      Code:
    </p>
    <pre class="reference">@software{stevens2025saev,
    title = {{saev}}, 
    author = {Stevens, Samuel and Wei-Lun Chao and Tanya Berger-Wolf and Yu Su},
    license = {MIT},
    url = {https://github.com/osu-nlp-group/saev}
}</pre>
    <p>
      Preprint:
    </p>
    <pre class="reference">@misc{stevens2025rigorous,
      title={Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models}, 
      author={Samuel Stevens and Wei-Lun Chao and Tanya Berger-Wolf and Yu Su},
      year={2025},
      eprint={2502.06755},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.06755}, 
}</pre>
    <h2>Acknowledgements</h2>
    <p>
      We would like to thank our colleagues from the Imageomics Institute, the ABC Center, and the OSU NLP group for their insightful and valuable feedback.
    </p>
    <p>
      This work was supported by the <a href="https://imageomics.org">Imageomics Institute</a>, which is funded by the US National Science Foundation's Harnessing the Data Revolution (HDR) program under <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2118240">Award #2118240</a> (Imageomics: A New Frontier of Biological Information Powered by Knowledge-Guided Machine Learning).
      Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
    </p>
    </article>
  </main>
</body>
</html>
